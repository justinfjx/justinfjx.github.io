<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>吴恩达机器学习课程笔记 | 第1-15章 | Justin</title><meta name="keywords" content="python,人工智能,机器学习"><meta name="author" content="Justin"><meta name="copyright" content="Justin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="吴恩达机器学习课程笔记 | 第1-15章"><meta name="application-name" content="吴恩达机器学习课程笔记 | 第1-15章"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="吴恩达机器学习课程笔记 | 第1-15章"><meta property="og:url" content="https://blog.jiaxin.fan/articles/20220130143422.html"><meta property="og:site_name" content="Justin"><meta property="og:description" content="@TOC 1 介绍1-3 监督学习 包括线性回归和逻辑回归  1-4 无监督学习 无监督学习不需要给数据打上标签，也就是不需要人告诉机器一部分正确答案是什么  2 单变量线性回归2-2 代价函数  Hypothesis即假定函数，是线性回归时机器推测出来的对已经给定的一堆离散点进行拟合之后的"><meta property="og:locale" content="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]"><meta property="og:image" content="https://blog.jiaxin.fan/img/default_cover.jpg"><meta property="article:author" content="Justin"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://blog.jiaxin.fan/img/default_cover.jpg"><meta name="description" content="@TOC 1 介绍1-3 监督学习 包括线性回归和逻辑回归  1-4 无监督学习 无监督学习不需要给数据打上标签，也就是不需要人告诉机器一部分正确答案是什么  2 单变量线性回归2-2 代价函数  Hypothesis即假定函数，是线性回归时机器推测出来的对已经给定的一堆离散点进行拟合之后的"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="https://blog.jiaxin.fan/articles/20220130143422.html"><link rel="preconnect" href="//cdn.cbd.int"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/swiper/swiper.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: undefined,
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: {"enable":true,"ck":"3M2ZO31o4YGc4oFd","LingQueMonitorID":"3M2ZO31o4YGc4oFd"},
  greetingBox: {"enable":true,"default":"哈喽！！！👋","list":[{"greeting":"晚安😴","startTime":0,"endTime":5},{"greeting":"早上好鸭👋, 祝你一天好心情！","startTime":6,"endTime":9},{"greeting":"上午好👋, 状态很好，鼓励一下～","startTime":10,"endTime":10},{"greeting":"11点多啦, 在坚持一下就吃饭啦～","startTime":11,"endTime":11},{"greeting":"午安👋, 加油！","startTime":12,"endTime":14},{"greeting":"🌈充实的一天辛苦啦！","startTime":14,"endTime":18},{"greeting":"19点喽, 奖励一顿丰盛的大餐吧🍔。","startTime":19,"endTime":19},{"greeting":"晚上好👋, 在属于自己的时间好好放松😌~","startTime":20,"endTime":24}]},
  twikooEnvId: 'https://justinfjx.netlify.app/.netlify/functions/twikoo',
  commentBarrageConfig:undefined,
  music_page_default: "nav_music",
  root: '/',
  preloader: undefined,
  friends_vue_info: {"apiurl":null},
  navMusic: false,
  mainTone: {"mode":"api","api":"https://img2color-go.vercel.app/api?img=","cover_change":true},
  authorStatus: {"skills":["🤖️ 数码科技爱好者","🔍 分享与热心帮助","🏠 智能家居小能手","🔨 设计开发一条龙","🤝 专修交互与设计","🏃 脚踏实地行动派","🧱 团队小组发动机","💢 壮汉人狠话不多"]},
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"距离本文上次更新已有","messageNext":"天啦！图文如有失效可以邮件提醒我更新哦~"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: Justin","link":"链接: ","source":"来源: Justin","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'Justin',
  title: '吴恩达机器学习课程笔记 | 第1-15章',
  postAI: '',
  pageFillDescription: '1 介绍, 1-3 监督学习, 1-4 无监督学习, 2 单变量线性回归, 2-2 代价函数, 2-5 Batch 梯度下降算法, 4 多变量线性回归, 4-1 多特征, 4-2 多元梯度下降法, 4-3 多元梯度下降法I——特征缩放, 缩放, 归一化, 4-4 多元梯度下降法演练I——学习率α, 4-5 特征和多项式回归, 4-6 正规方程（Normal Equation）（区别于迭代方法的直接解法）, 6 逻辑回归, 6-1 逻辑分类算法, 6-2 假设表示, 6-3 决策边界, 6-4 代价函数, 6-4 简化代价函数与梯度下降, 6-5 高级优化, 6-7 多元分类：一对多, 7 正则化, 7-1 过拟合问题, 7-2 (正则化更改)代价函数, 7-3 线性回归的正则化, 7-4 逻辑回归的正则化, 8 神经网络的表示, 8-1 非线性假设, 8-2 神经元与大脑, 8-3 前向传播-模型展示I, 8-4 前向传播-模型展示II, 8-5 例子与理解I, 8-6 例子与理解II, 8-7 多元分类, 9 神经网络：Learning, 9-1 应用于神经网络的代价函数, 9-2 反向传播算法, 9-3 理解反向传播, 9-4 展开参数, 9-5 梯度检测, 9-6 随机初始化, 9-7 回顾总结, 10 应用机器学习的建议, 10-1 决定下一步做什么, 10-2 评估假设函数, 10-3 模型选择和训练、验证、测试集, 10-4 判断偏差与方差, 10-5 正则化和偏差、方差, 10-6 学习曲线, 10-7 决定接下来做什么, 11 机器学习系统设计, 11-1 确定执行的优先级：以垃圾邮件分类为例, 11-2 误差分析, 11-3 不对称性分类的误差评估, 11-4 查准率和召回率的平衡, 11-5 机器学习数据, 12 支持向量机（SVM）, 12-1 优化目标, 12-2 大间距的理解, 12-3 支持向量机的数学原理, 12-4 核函数I, 12-5 核函数II, 12-6 使用支持向量机(SVM), 13 聚类算法, 13-1 无监督学习, 13-2 K均值(K-means)算法, 13-3 优化目标, 13-4 随机初始化(K均值聚类算法), 13-5 如何选择聚类数量K, 14 降维, 14-1 目标I：数据压缩, 14-2 目标II：可视化, 14-3 主成分分析方法（PCA）, 14-4 主成分分析算法（PCA）, 14-5 压缩重现（解压缩）, 14-6 选择主成分数量, 14-7 应用PCA的建议, 15 异常检测, 15-1 问题动机, 15-2 高斯分布（正态分布）, 15-3 算法, 15-4 开发和评估异常检测系统, 15-5 异常检测vs监督学习, 15-6 选择要使用的特征, 15-7 多元高斯分布, 15-8 使用多元高斯分布的异常检测介绍监督学习包括线性回归和逻辑回归无监督学习无监督学习不需要给数据打上标签也就是不需要人告诉机器一部分正确答案是什么单变量线性回归代价函数即假定函数是线性回归时机器推测出来的对已经给定的一堆离散点进行拟合之后的函数和是待求的参数求出这两个参数后就能得出一条直线进行拟合即代价函数是对数据集中每个点与假定函数进行作差再平方再相加后求得的平均值再乘得到的乘只是为了之后求导方便式中为数据集中数据个数共有个指的是数据集第行的的值指的是数据集第行的的值即为需要通过机器学习推测的值目标是求代价函数的最小值并求出此时和的值因为在代价函数的值最小时假定函数距离各离散点的距离最近由于有两个参数和所以得出的代价函数图形是一个如上图所示的曲面图曲面图也可以用上图的等高线图表示梯度下降算法梯度下降算法用于如上图的函数在图上取一个点向四周寻找最快的下山方向并迈出一步重复执行上述步骤就可以找到代价函数的最小值上图仅用于解释梯度下降算法线性回归的代价函数永远是一个凸函数只会存在一个全局最优解不会像上图一样出现局部最优解和全局最优解在这里是赋值符号是两边等价的意思如上图和需要同步更新来实现梯度下降是学习率越大相当于下山的步子越大下山就越快如上图从右侧较大时开始逐渐向左侧递减是常数不改变代价函数的导数项这个导数项其实是偏导数会随着代价函数逐渐降低而变小因为斜率变小了在到达最小值时导数会变为将梯度下降式子里的导数项求出后得到上面的式子多变量线性回归多特征用来表示数据集中特征的数量这里有个特征表示输出量用来表示第行的数据输入量用来表示第行的第个特征上图是新的假定函数是多个特征可以假定一个这样和就可以写成两个列向量假定函数就可以写作即求两个矩阵的内积多元梯度下降法如上图从到同步更新同时特征需要对应多元梯度下降法特征缩放缩放这里假设只有两个特征值特征的取值范围是特征的取值范围是之后作出的代价函数的等高线图会是一个又高又瘦的椭圆在进行梯度下降算法时可能会反复震荡导致收敛太慢如上图左侧将特征和缩放使这两个特征值的范围都在这样产生的代价函数的图像会变成向上图右侧一样的圆形这样就会更快地收敛一般会让特征值处于范围内如果比较接近也可以直接计算不需要进行特征缩放如果特征值过小如也需要进行缩放归一化如特征的平均值为范围为则可以将化为在这里的例子里是房屋的面积公式为为特征原来的范围大小为特征原来的平均值多元梯度下降法演练学习率上图左侧纵坐标为计算得出的代价函数的值横坐标为进行梯度下降算法的次数次数增加后代价函数会越来越接近最小值逐渐收敛如果图像为上图左侧的两种说明学习率过大导致像上图右侧一样的变化发散只要学习率足够小一定会是收敛的即进行梯度下降算法的次数越多代价函数一定会越来越接近最小值学习率过小会导致收敛速度变慢取学习率时通常是每隔大约三倍取如特征和多项式回归用多次的函数来拟合依然使用之前的一次式但让或者也可以令假定函数是正规方程区别于迭代方法的直接解法最小二乘法使用这个式子不需要进行特征缩放为的矩阵为数据数量为特征个数上图是梯度下降算法和正规方程的优缺点在特征数量大于左右时开始考虑不使用正规方程而使用梯度下降算法逻辑回归逻辑分类算法用于输出量是离散值如的情况不宜用线性回归进行机器学习假设表示在逻辑分类中将假定函数改为假定函数函数的值即为输出真的概率函数称作函数或函数其表达式为综上逻辑分类中的假定函数为决策边界在上图右上角函数的图像中用来表示假定函数要计算何时只要计算何时即计算何时计算得出的决策边界可以是直线决策边界也可以是圆形的代价函数之前的线性表示的代价函数为将表示为去掉之后简写为由于在逻辑回归上继续使用这个代价函数会导致输出的代价函数不是凸函数不能使用梯度下降算法输出全局最小值所以在逻辑回归中将代价函数中的改为如下中第一个函数的图像如上图输出量时若假定函数那么输出值将会趋向于即代价函数趋向于使得这点对假定函数的惩罚为几乎不改变结果输出量时若假定函数那么输出值将会趋向于即代价函数趋向于使得这点对假定函数的惩罚为极大地改变结果使假定函数在这点处的输出结果迅速回到正确的值上来中第二个函数的图像如上图简化代价函数与梯度下降由于恒成立所以可以将简化为一个式子最后得出的用于逻辑回归的代价函数为在梯度下降中不断循环的式子为高级优化多元分类一对多共有三个分类三角形叉正方形先计算三角形将三角形认为是正样本其他认为是负样本可以计算出一条直线划分三角形和其他图形如上图右上角第一幅坐标系其他图形同理正则化过拟合问题上图左侧坐标系为欠拟合用一条直线不能很好的表示这个数据集偏差很大上图中间为合适的拟合上图右侧为过度拟合拟合的曲线波动很大假定函数中的变量过多虽然代价函数非常接近但不能泛化到其他数据集中上图为在逻辑回归中的欠拟合合适的拟合过度拟合解决过拟合减少特征数量正则化正则化更改代价函数使用正则化更改代价函数直接使用上图右侧的假定函数及其代价函数会导致过拟合在不去掉和的前提下可以在代价函数上加上对于和的惩罚项只是随便一个比较大的数加上惩罚项之后的代价函数在运算中会让参数和变得尽可能小让和对假定函数的图像影响变小这样就可以在保留和参数的情况下不产生过拟合加上惩罚项后的假定函数变为由于我们一般不知道哪一项会导致过拟合所以在代价函数中加入正则化项加入后的代价函数为为正则化参数一般不对增加惩罚项所以从开始线性回归的正则化正则化后的梯度下降循环项如上图由于在代价函数中没有添加对的惩罚项所以对的更新分开表示对于的梯度下降项也可化简为上图的式子可以得到是一个小于但非常接近的数使用正则化后得到的正规方程为这里只要那么括号内计算得出的矩阵一定可逆而在原来没有用正则化得出的正规矩阵中如果为样本数为特征数那么括号内矩阵不可逆是奇异矩阵所以正则化也可以应用于样本数小于特征数时的情况让括号内矩阵可逆逻辑回归的正则化正则化后逻辑回归的代价函数改为正则化后的梯度下降循环项如上图神经网络的表示非线性假设对于一幅图像来说如果取出每个像素点的灰度值或其他特征表示方法来作为一个数据样本的话数据集将会非常庞大如果运用之前的回归算法来计算将会产生非常大的计算成本神经元与大脑前向传播模型展示上图指一个带有激活函数的人工神经元在神经网络术语中称作激活函数神经网络指的其实是一组神经网络的集合第一层称为输入层第二层称为隐藏层第三层称为输出层用来表示第层的第个神经元的激活项所谓激活项是指由一个具体的神经元计算并输出的值用表示从第层到第层的权重矩阵参数矩阵就是之前的矩阵之前的既可以叫做参数也可以叫做权重和的计算公式在上图中已写出其中是一个的矩阵如果神经网络在第层有个单元在第层有个单元那么是一个的矩阵前向传播模型展示前向传播的矢量化实现把上面式子中的表示为则扩展到全域第二层的激活值其中另外需要添加偏置项例子与理解例子与理解上图为计算的神经网络第一层到第二层先计算得到计算得到再以和为和计算得出的结果即为多元分类这里有四种输出所以共有四个输出单元输出的为一个维矩阵可能是或或或中的其中一个分别表示或或或神经网络应用于神经网络的代价函数用表示神经网络的总层数用表示第层单元神经元的数量不包括偏置单元为维向量即神经网络输出层共有个神经元即有个输出表示第个输出应用于神经网络的代价函数为第二项中的指的是将行列的矩阵中的每一个元素相加起来第二项中的指的是将输入层和隐藏层的矩阵都求和反向传播算法定义为第层第个神经元的偏差以上图的四层的神经网络为例指第个输出在数据集中的值指神经网络的第个输出也可表示为用向量方法表示上式可表示为也可表示为其中其中点乘结果是一个数叉乘结果是一个向量这里忽略了正则化项即认为上图是反向传播算法的流程最后可以得到然后进行梯度下降算法理解反向传播以上图的神经网络为例展开参数梯度检测要估计代价函数上点处的导数值可以运用为宜求得导数扩展到向量中如上图是一个维向量是矩阵的展开可以估计的值将估计得到的偏导数值与反向传播得到的偏导数值比较如果两个值非常近就可以验证计算是正确的一旦确定反向传播算法计算出的值是正确的就应该关掉梯度检验算法随机初始化如果在程序开始时令中所有元素均为会导致多个神经元计算相同的特征导致冗余这成为对称权重问题所以在初始化时要令等于中的一个随机值回顾总结训练一个神经网络随机一个初始权重执行前向传播算法得到对所有的计算代价函数执行反向传播算法计算通过梯度检验算法得到估计的的偏导数值将估计得到的偏导数值与反向传播得到的偏导数值比较如果两个值非常近就可以验证反向传播算法的计算结果是正确的验证完后关闭梯段检验算法运用梯度下降算法或其他更高级的优化方法结合反向传播计算结果得到使最小时的参数的值应用机器学习的建议决定下一步做什么评估假设函数随机选择数据集中的作为训练集作为测试集将数据集分为两个部分首先根据训练集计算出参数把参数代入测试集计算代价函数的值这里是线性回归的代价函数逻辑回归的代价函数同理逻辑分类中有另一种形式的测试度量称作错误分类或错误分类计算过程如上图模型选择和训练验证测试集随机选择数据集中的作为训练集作为交叉验证集验证集作为测试集将数据集分为三个部分上图计算训练误差验证误差和测试误差用来表示假定函数的多项式的最高次幂先用训练集求每一个假定函数的代价函数取到最小值时的值再把这个求得的代入交叉验证集求得对每一个假定函数进行以上步骤的计算得到的个代价函数的值取最小的那个这里假定是再使用时的代入测试集得到泛化误差判断偏差与方差上图坐标系轴为误差轴为多项式最高次幂的大小以上图为例当过小时出现欠拟合偏差过大此时的训练集误差很大并且当过大时出现过拟合方差过大此时的训练集误差很小并且正则化和偏差方差勘误正则化项里面上面的应该改为若正则化参数过大会导致欠拟合高偏差并且参数假定函数的结果如上图最左侧坐标系若正则化参数过小会导致过拟合高方差如上图最右侧坐标系使用不带正则化项的取下图中的带正则化项的代价函数首先取多种的情况这里从开始下一个是上一个的两倍一直取到由于此时小数部分对结果影响不大所以也可以直接取将每一种情况的代入带正则化项的代价函数中这里共能得到个不同的值再把这个代入到交叉验证集代价函数中注意这里的是不带正则化项的最后得到个不同的值取最小的那个值对应的这里认为第个是最合适的代入到测试集的代价函数中计算泛化误差注意这里的也是不带正则化项的学习曲线如上图是高偏差欠拟合的情形此时增加数据集数量对于误差的缩小没有明显帮助如上图是高方差过拟合的情形此时交叉验证集的曲线和训练集的曲线中间相差较大所以增加数据集数量对于减小误差是有帮助的决定接下来做什么解决高偏差或高方差的一些方法如下图如上图在一个小型的神经网络中容易出现欠拟合现象但它的好处是计算量较小在一个大型的神经网络中容易出现过拟合现象可以通过正则化来避免它的好处是效果较好但是计算量较大机器学习系统设计确定执行的优先级以垃圾邮件分类为例误差分析不要在编写程序时进行过早的优化在编写完整个系统后再根据学习曲线优化系统先编写一个简单粗暴的算法再根据输出的结果改进算法误差分析观察被错误分类的数据有何共同的特征以垃圾邮件分类为例查看被错误分类的垃圾邮件有何共同的特征或以数值方式表示误差推荐在交叉验证集上做误差分析不对称性分类的误差评估偏斜类问题一个数据集中的一类数据样本比另外一类的样本少的多比如占样本的而占样本的在偏斜类问题中比如患癌症占样本的而不患癌症占样本的假设一个算法达到了的预测准确率这时已经只有的误差但是如果始终预测不患癌症也有的准确率误差也只有这时需要用到不同的误差度量值其中一种叫做查准率和召回率左侧表格上方为实际的表格左侧为预测的代表患癌代表不患癌如果预测为实际为则是真阳性如果预测为实际为则是假阳性如果预测为实际为则是假阴性如果预测为实际为则是真阴性查准率预测为真并且实际为真的数量预测为真的数量召回率预测为真并且实际为真的数量实际为真的数量两者均是越高越好查准率和召回率的平衡本来在超过概率时预测为即患癌在低于时预测为即不患癌现在如果要在超过患癌可能性时才预测为患癌的话会得到一个高查准率但低召回率的模型如果要在超过患癌可能性时就预测患癌会得到一个高召回率但低查准率的模型高查准率能不揪出来的就不揪出来高召回率能揪出来的都揪出来通过计算查准率和召回率的平均值不能直接评估一个算法的好坏通过值值能更好地评估一个算法的好坏为查准率为召回率机器学习数据在有一个非常庞大的训练集的前提下即使有很多的参数训练集数量参数数量也能很好地拟合数据集不会产生过拟合现象支持向量机优化目标上图左侧坐标系为时代价函数的图像支持向量机时画出粉色的曲线命名为下标指的值为同样的右侧坐标系为时代价函数的图像支持向量机时画出粉色的曲线命名为下标指的值为在逻辑回归中代价函数为要支持向量机先将上式里的负号放到求和里面去然后把去掉是一个常数虽然去掉它会改变代价函数的值但是仍然能求得一样的最小值对应的之后得到的代价函数为把上式中的替换为把替换为得到代价函数在支持向量机中不再使用正则化参数改为使用参数更改后的支持向量机的代价函数为支持向量机中不会预测的概率如果假定函数输出反之输出大间距的理解支持向量机又被称为大间距分类器更改支持向量机中的判断边界让时才输出时才输出这样两个结果之间就有一个安全间距使用一般的逻辑回归算法可能会生成上图中粉色和绿色的直线来分割两类样本而使用支持向量机会生成图中黑色的直线图中两条蓝线中间的区域称为间距支持向量机会尽可能的将两种样本以最大的间距分隔开可以看出支持向量机后可以有更好地鲁棒性如上图先假设没有左侧的那一个负样本用一个很大的可以生成上图中黑色直线但如果有左侧的那一个负样本由于很大支持向量机为保证两类样本间有最大的间距会生成上图中粉色的直线但如果没有那么大则即使有左侧的那一个负样本依然会生成黑色直线就相当于之前的虽然两者确实不一样但呈现出来的效果差不多支持向量机的数学原理核函数如图的样本如果假定函数就预测其他情况预测在上图中的假定函数中设假定函数变为以上方法与下文方法无关与核函数无关然而除了对原有的特征进行组合以外有没有更好的方法来构造我们可以利用核函数来计算出新的特征在坐标系中取个样本标记矩阵是一个给定的训练实例假设有两个特征值那么令称为相似度量函数核函数也可写为表示这里称为高斯核函数是高斯核函数的参数核函数运算时可化为如果非常接近标记那么如果远离标记那么上图如果变大那么的下降速度会变慢斜率降低如上图取一点已经通过支持向量机计算得出的值如上图然后可以按照核函数计算出的值如上图将和的值代入假定函数得到所以预测如上图再取另外的点假设最后得到靠近和的点会预测为而远离和的点会预测为那么最后可以拟合出一条如图中红色的曲线曲线内预测为曲线外预测为核函数如何选择地标我们通常是根据训练集的数量选择地标的数量即如果训练集中有个实例则我们选取个地标并且令这样做的好处在于现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的即由将写为特征向量形式得到是一个维矩阵因为除了个样本外还加入了一个偏置项矩阵的含义是第个样本里的所有特征与从到的每一个样本里的所有特征进行核函数运算一共个运算结果排列在矩阵里再加上第个的用替换带项得到代价函数为关于正则化项中的解释是对应于矩阵的权重由于一共有共个对应着个所以假定函数中一共有项即有个而之前已经规定了权重的个数用来表示所以这里有注意这里已经忽略了不对其进行正则化正则化项在具体实施中求和部分可写为并且在计算时用代替矩阵是根据我们选择的核函数而不同的一个矩阵这样做可以提高计算效率修改后的代价函数为理论上讲我们也可以在逻辑回归中使用核函数但是上面使用来简化计算的方法不适用于逻辑回归因此计算将非常耗费时间在此我们不介绍最小化支持向量机的代价函数的方法你可以使用现有的软件包如等在使用这些软件包最小化我们的代价函数之前我们通常需要编写核函数并且如果我们使用高斯核函数那么在使用之前进行特征缩放是非常必要的另外支持向量机也可以不使用核函数不使用核函数又称为线性核函数当我们不采用非常复杂的函数或者我们的训练集特征非常多而实例非常少的时候可以采用这种不带核函数的支持向量机下面是支持向量机的两个参数和的影响较大时相当于较小可能会导致过拟合高方差较小时相当于较大可能会导致低拟合高偏差较大时可能会导致低方差高偏差较小时可能会导致低偏差高方差来自上图是两个参数对结果的影响使用支持向量机使用软件库来计算的值我们需要选择的值我们需要选择核函数线性内核函数不选择核函数直接使用线性拟合可以在特征数量很大而样本数量很小的情况下使用高斯核函数可以在样本数量很大特征数量很小的情况下使用能够拟合出非线性边界使用非线性核函数时需要将特征值归一化核函数需要满足默塞尔定理聚类算法无监督学习无监督学习的数据集是一堆不带标签的数据他们没有的值只有的值均值算法均值算法的第一步簇分配确定两个聚类中心图中蓝色叉和红色叉遍历每一个样本图中绿点判断离哪个聚类中心更近将样本分为两个簇分完之后如下图均值算法的第二步移动聚类中心计算每一簇中所有点的均值并将聚类中心移动到该均值处移动后如下图然后再重复第一步判断每一个样本离哪个聚类中心近并改变他的颜色分类改变后再重复第二步这样不断重复得到最终结果这样就可以说均值已经聚合了输入一个表示想要将数据分为几类输入不带标签的训练集设训练集是一个维向量按照惯例不考虑这一项如上图用表示想要将数据分为类用表示第个聚类中心的位置他是一个向量矩阵随机初始化获得用表示样本中第个点距离最近的那个聚类中心的下标即第个样本距离第个聚类中心最近即第个样本属于第个聚类中心求法如上图中蓝色笔迹求完上述值后计算每一个聚类中心包含的点的均值赋值给对应的此时已经得到新的聚类中心的位置如果有一个没有点的聚类中心一般直接移除这样最后会得到类但如果确实是需要分为类那么就将那个没有点的聚类中心重新随机初始化如上图有时均值算法也运用于不能很明显的分类的数据集如收集了很多人的身高体重作为数据集可以看出这些数据基本是连续的要将其分为三类用聚类算法也能分为三类聚类算法也可用于市场的分割优化目标表示第个样本所属聚类中心的位置均值聚类算法的代价函数优化目标函数为表示的是每一个样本的位置与它所属的聚类中心位置作差取范数再平方所有共个样本加起来再求平均数这个代价函数有时也称为失真代价函数或均值算法的失真随机初始化均值聚类算法从训练集中随机挑选个样本让第一到第的聚类中心等于刚刚随机出来的个样本如上图由于是随机挑选的聚类中心所以结果可能是全局最优如上图上面的坐标系也可能落在局部最优上如上图下面两个坐标系所以采用多次随机初始化的方法寻找全局最优如上图多次随机初始化的方法为运行次均值聚类算法可以得到许多不同的代价函数的值取最小的那个就是最优的聚类如果到那么多次随机初始化可以明显改进聚类算法效果如果大于多次运行可能不会有特别明显的改善如何选择聚类数量一般是手动选择如上图运用肘部法则坐标系轴为聚类数量坐标系轴为代价函数的值绘制曲线后如左侧的坐标系能看到曲线在处由很高的斜率转为了很低的斜率这一点认为是肘部选择是合适的但也有可能曲线是如右边坐标系的图像不能明确地找出合适的聚类数量或者另外一种方法根据下游目的来手动选择聚类数量降维目标数据压缩如上图将数据从二维压缩为一维以表示同一个物体的长度为例为用厘米表示为用英尺表示由于四舍五入产生的误差坐标系中的样本坐标没有完全练成一条直线对其进行线性拟合得到一条直线让这些点投影在另一条坐标轴上这样可以用一个一维的数字来表示原来的一个二维向量如上图将数据从三维压缩为二维在实际应用中可能是将维的数据压缩为维空间中所有的点几乎都在一个平面的周围将所有的点投射到这个平面上用和来表示平面的两个坐标轴这样就把一个三维空间压缩为二维平面原来的数据用一个二维向量即可表示中有两个特征和降维后的数据可以提高学习算法的运算效率并且节省磁盘存储空间目标可视化一般取来可视化数据集主成分分析方法有这样的一个数据集这个数据集含有二维实数空间内的样本假设我想对数据进行降维从二维降到一维也就是说我需要找到一条直线将数据投影到这条直线上上图中红线是一个不错的选择因为每个点投影到直线上的距离蓝线都很短所以就是寻找一个低维的东西在这个例子中是一条直线让数据投射在上面时的距离的平和最小这个距离被称为投影误差在使用钱需要先进行均值归一化和特征规范化使得特征和均值为数值在可比较的范围之内由二维到一维时找到一个向量即可三维到二维时需要找到个向量组成一个平面更高维时需要找到个向量让样本投影到这个向量展开的线性子空间上上图解释了与线性回归的不同线性回归是左侧的坐标系他对一条条竖直的与轴平行的蓝线求和因为线性回归计算出的误差是指预测的值与实际值之间的差是右侧的坐标系他对一条条垂直于降维后的直线在这里是直线的蓝线求和因为计算的是实际的点与降维后直线的距离实际的点是投影上去的主成分分析算法首先进行数据预处理进行均值标准化可能要进行特征缩放均值标准化按照上图先求出某个特征在所有样本中的平均值公式为然后把每一个旧的替换成这样每一个特征的均值都为先计算矩阵协方差计算公式为表示为矩阵形式为然后用软件库调用算法得到矩阵是一个的矩阵这里的因为共有个样本数量取矩阵的前列就是要降维成的维空间里的个向量空间是几维就需要几个向量来表示这个空间如三维降二维时需要两个向量来表示二维空间如上图把刚刚取出的个列向量组成的矩阵命名为则得到的低维维的数据集该数据集是一个维向量压缩重现解压缩之前进行了这样的运算其中是新得到的一维向量是原来的二维向量是通过算法得出的现在要恢复二维进行这样的运算选择主成分数量上图中分子的式子称为平均平方映射误差分母称为数据的总变差它的意思是平均来看我的训练样本距离零向量多远平均来看我的训练样本距离原点多远分数计算的结果为降维后的新数据与原数据的差距有多大比如假设结果则可以说有的差异这个数字比较典型的取值为甚至也可能是上图左侧是计算合适的值的方法这里假设与原数据有小于等于的误差可以直接调用算法其中输出的矩阵是一个对角阵用公式直接判断这个公式是否成立即可找到让这个公式成立的的值就是合适的的取值或者用来判断也是一样的即使要手动选择值计算出差异值也可以帮助向别人解释你实现的的性能的一个好方法熟悉的人们就可以通过它来更好地理解你用来代表原始数据的维数据近似得有多好因为有的差异性被保留了应用的建议在使用监督学习时也可以运用来增加运算效率先将从原来的样本中抽出运用算法将其降维得到然后把降维后的替换到原来的样本中与一一对应然后进行监督学习的算法注意只能在训练集中使用不能用于交叉验证集和测试集从训练集得到了到的对应关系后可将这个对应关系应用到交叉验证集和测试集不要用来防止过拟合更好的方法是用正则化是在丢失一定精度的境况下提高运算效率它在降维时没有与相关在使用之前首先尝试使用原数据进行运算只有在运算速度过慢占用内存太大占用磁盘太大原数据无法成功计算时才使用异常检测问题动机以飞机发动机的异常检测为例和分别表示发动机的两个特征先有一堆数据集表示正常的发动机如上图红色叉可以认为越靠近圆圈中间越正常现在有一个新的发动机将他放进坐标系中比较越靠近中心表示越大设定一个阈值如果则表示该新发动机正常如果则表示发动机异常异常检测常被用来进行用户的欺诈监测检测异常的用户高斯分布正态分布是实数表示以分布表示按正态分布表示均值表示方差如上图正态分布曲线是一条钟形曲线纵轴表示取到的概率决定曲线中心的位置标准差决定曲线的宽度高斯分布曲线的公式高斯分布曲线的积分即上图红色阴影部分面积如上图现有一个无标签数据集可以看出数据集在中间概率大两侧概率小假设满足正态分布进行参数估计假设数据集满足正态分布参数估计公式为求方差的公式中也可能是在机器学习中这两种公式差距不大算法密度估计问题表示特征出现的概率假设所有特征均满足正态分布给出概率异常检测算法的流程如上图开发和评估异常检测系统假设有一个带标签的数据集选取出正常的样本去掉标签作为训练集得出异常检测算法然后用交叉验证集带标签来验证用测试集带标签来测试上图是以发动机检测为例的训练集交叉验证集和测试集的分法由于数据集中的数据可能非常倾斜所以可能需要计算值来判断算法的效果或者用值来决定怎样的是合适的异常检测监督学习异常检测算法通常用于有很少数量的正样本这里以异常的飞机发动机为例有很大数量的负样本这里以正常的飞机发动机为例学习算法很难从极少数数量的正样本中学习出飞机发动机的异常之处但能根据负样本学习出正态分布曲线未来的异常发动机出现的异常特征难以预测可能与当前的正样本中的异常特征完全不同监督学习算法通常用于有很大数量的正样本和负样本有足够的的正样本数量来让学习算法找到特征并且未来出现的正样本的特征与目前已有的样本的特征相似样本数量与某类情况出现概率没关系即使一类情况出现概率为另一类概率为只要同时有大量的正样本和负样本就能用监督学习算法比如癌症预测选择要使用的特征如果数据像上图第一行的坐标系一样看起来像正态分布的话可以直接用正态分布如果数据像上图第二行左侧坐标系不符合正态分布的话可以对特征进行转换比如这里用来替换特征值让新得到的曲线像正态分布然后用正态分布除了用之外也可以用是常数等等如果如上图一个绿色的异常样本混在了正常样本中算法没能将其挑出来可以寻找创造一个新的特征让这个异常样本与其他正常样本区别开来举例服务器负载量和网络流量应该是线性关系的一起变大一起变小如果某一台服务器陷入了死循环就会导致负载很大而网络流量很小的情况这种情况下要找出异常可以创建一个新的特征负载网络流量这样就能捕捉到异常或者是负载的平方也可以多元高斯分布以服务器的异常为例上图中绿色样本在两轴中的概率都不低但很明显在左侧坐标系中他是一个异常样本算法不能意识到左侧坐标系中的椭圆内的才是正常样本而默认是以样本中心为圆心按圆向外概率递减所以要使用多元高斯分布将这两个特征合起来考虑一个维向量和一个的协方差矩阵作为多元高斯分布的参数公式为其中是矩阵的行列式表示的是高斯分布图像中的峰值的位置这里第一个数是特征的位置第二个数是特征的位置和的计算公式会在下一节给出对图像的影响如上图所示使用多元高斯分布的异常检测和的计算公式如上图首先计算出和的值然后代入到多元高斯分布计算公式计算出概率然后将概率与比较判断正常异常如上图原来的高斯分布计算出的正态分布图像是一种特殊的多元高斯分布图像他的椭圆是与轴平行的而多元高斯分布可以生成一个不与轴平行的椭圆图像原始高斯分布模型与多元高斯分布的应用区别原来的可能需要手动创造特征来正确识别异常项而多元高斯分布不需要它能自动捕捉这些特征原来的计算量较小当特征量达到时就需要用原来的了原来的可以在样本数量很小的情况下正常工作而多元高斯分布需要保证一般是否则矩阵会是一个不可逆矩阵在多元高斯分布中如果几个特征之间是线性相关的也会导致矩阵会是一个不可逆矩阵比如或者可以通过删除特征来解决机器学习吴恩达机器学习系列课程',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-01-07 09:57:47',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><div class="back-home-button"><i class="anzhiyufont anzhiyu-icon-grip-vertical"></i><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" href="https://blog.jiaxin.fan/" title="我的博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/202504281806460.jpeg" alt="我的博客"/><span class="back-menu-item-text">我的博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div></div><a id="site-name" href="/" accesskey="h"><div class="title">Justin</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 时光</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/link/"><span> 导航</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/comments/"><span> 留言板</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/air-conditioner/"><span> 小空调</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="javascript:toRandomPost()"><span> 随便逛逛</span></a></div></div></div><div id="nav-right"><div class="nav-button only-home" id="travellings_button" title="随机前往一个开往项目网站"><a class="site-page" onclick="anzhiyu.totraveling()" title="随机前往一个开往项目网站" href="javascript:void(0);" rel="external nofollow" data-pjax-state="external"><i class="anzhiyufont anzhiyu-icon-train"></i></a></div><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> 搜索</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/Hexo/" style="font-size: 1.05rem;">Hexo<sup>1</sup></a><a href="/tags/anaconda/" style="font-size: 1.05rem;">anaconda<sup>1</sup></a><a href="/tags/axios/" style="font-size: 1.05rem;">axios<sup>1</sup></a><a href="/tags/csdn/" style="font-size: 1.05rem;">csdn<sup>1</sup></a><a href="/tags/cuda/" style="font-size: 1.05rem;">cuda<sup>1</sup></a><a href="/tags/c%E8%AF%AD%E8%A8%80/" style="font-size: 1.05rem;">c语言<sup>3</sup></a><a href="/tags/docker/" style="font-size: 1.05rem;">docker<sup>1</sup></a><a href="/tags/ide/" style="font-size: 1.05rem;">ide<sup>1</sup></a><a href="/tags/latex/" style="font-size: 1.05rem;">latex<sup>1</sup></a><a href="/tags/linux/" style="font-size: 1.05rem;">linux<sup>4</sup></a><a href="/tags/markdown/" style="font-size: 1.05rem;">markdown<sup>1</sup></a><a href="/tags/matlab/" style="font-size: 1.05rem;">matlab<sup>2</sup></a><a href="/tags/numpy/" style="font-size: 1.05rem;">numpy<sup>1</sup></a><a href="/tags/nvidia/" style="font-size: 1.05rem;">nvidia<sup>3</sup></a><a href="/tags/opencv/" style="font-size: 1.05rem;">opencv<sup>3</sup></a><a href="/tags/openmv/" style="font-size: 1.05rem;">openmv<sup>1</sup></a><a href="/tags/openvino/" style="font-size: 1.05rem;">openvino<sup>1</sup></a><a href="/tags/overleaf/" style="font-size: 1.05rem;">overleaf<sup>1</sup></a><a href="/tags/pip/" style="font-size: 1.05rem;">pip<sup>1</sup></a><a href="/tags/pycharm/" style="font-size: 1.05rem;">pycharm<sup>1</sup></a><a href="/tags/python/" style="font-size: 1.05rem;">python<sup>14</sup></a><a href="/tags/pytorch/" style="font-size: 1.05rem;">pytorch<sup>4</sup></a><a href="/tags/stm32/" style="font-size: 1.05rem;">stm32<sup>3</sup></a><a href="/tags/ubuntu/" style="font-size: 1.05rem;">ubuntu<sup>4</sup></a><a href="/tags/vue-axios/" style="font-size: 1.05rem;">vue-axios<sup>1</sup></a><a href="/tags/vue-js/" style="font-size: 1.05rem;">vue.js<sup>2</sup></a><a href="/tags/vue3/" style="font-size: 1.05rem;">vue3<sup>1</sup></a><a href="/tags/windows/" style="font-size: 1.05rem;">windows<sup>4</sup></a><a href="/tags/%E4%B8%B2%E5%8F%A3%E9%80%9A%E4%BF%A1/" style="font-size: 1.05rem;">串口通信<sup>1</sup></a><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 1.05rem;">人工智能<sup>22</sup></a><a href="/tags/%E5%85%B6%E4%BB%96/" style="font-size: 1.05rem;">其他<sup>6</sup></a><a href="/tags/%E5%8D%95%E7%89%87%E6%9C%BA/" style="font-size: 1.05rem;">单片机<sup>2</sup></a><a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 1.05rem;">卷积神经网络<sup>1</sup></a><a href="/tags/%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%A1%AC%E4%BB%B6/" style="font-size: 1.05rem;">嵌入式硬件<sup>2</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">机器学习<sup>17</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">深度学习<sup>9</sup></a><a href="/tags/%E7%9F%A9%E9%98%B5/" style="font-size: 1.05rem;">矩阵<sup>1</sup></a><a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 1.05rem;">神经网络<sup>4</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 1.05rem;">计算机视觉<sup>7</sup></a><a href="/tags/%E9%95%9C%E5%83%8F/" style="font-size: 1.05rem;">镜像<sup>1</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/05/"><span class="card-archive-list-date">五月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/04/"><span class="card-archive-list-date">四月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/02/"><span class="card-archive-list-date">二月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">7</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">十月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/08/"><span class="card-archive-list-date">八月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/07/"><span class="card-archive-list-date">七月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/06/"><span class="card-archive-list-date">六月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/05/"><span class="card-archive-list-date">五月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/CV/" itemprop="url">CV</a><i class="anzhiyufont anzhiyu-icon-angle-right post-meta-separator"></i><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/CV/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" itemprop="url">吴恩达机器学习课程笔记</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/tags/python/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>python</span></a><a class="article-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>人工智能</span></a><a class="article-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>机器学习</span></a></span></div></div><h1 class="post-title" itemprop="name headline">吴恩达机器学习课程笔记 | 第1-15章</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2022-01-30T14:34:22.000Z" title="发表于 2022-01-30 22:34:22">2022-01-30</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2026-01-07T01:57:47.959Z" title="更新于 2026-01-07 09:57:47">2026-01-07</time></span></div><div class="meta-secondline"><span class="post-meta-separator"></span><span class="post-meta-wordcount"><i class="anzhiyufont anzhiyu-icon-file-word post-meta-icon" title="文章字数"></i><span class="post-meta-label" title="文章字数">字数总计:</span><span class="word-count" title="文章字数">13.8k</span><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-clock post-meta-icon" title="阅读时长"></i><span class="post-meta-label" title="阅读时长">阅读时长:</span><span>48分钟</span></span><span class="post-meta-separator"></span><span class="post-meta-pv-cv" id="" data-flag-title="吴恩达机器学习课程笔记 | 第1-15章"><i class="anzhiyufont anzhiyu-icon-fw-eye post-meta-icon"></i><span class="post-meta-label" title="阅读量">阅读量:</span><span id="busuanzi_value_page_pv"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></span><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为中国"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>中国</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="/img/default_cover.jpg"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="https://blog.jiaxin.fan/articles/20220130143422.html"><header><a class="post-meta-categories" href="/categories/CV/" itemprop="url">CV</a><a class="post-meta-categories" href="/categories/CV/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" itemprop="url">吴恩达机器学习课程笔记</a><a href="/tags/python/" tabindex="-1" itemprop="url">python</a><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" tabindex="-1" itemprop="url">人工智能</a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url">机器学习</a><h1 id="CrawlerTitle" itemprop="name headline">吴恩达机器学习课程笔记 | 第1-15章</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">Justin</span><time itemprop="dateCreated datePublished" datetime="2022-01-30T14:34:22.000Z" title="发表于 2022-01-30 22:34:22">2022-01-30</time><time itemprop="dateCreated datePublished" datetime="2026-01-07T01:57:47.959Z" title="更新于 2026-01-07 09:57:47">2026-01-07</time></header><span id="more"></span>



<p>@<a href="%E7%9B%AE%E5%BD%95">TOC</a></p>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><h2 id="1-3-监督学习"><a href="#1-3-监督学习" class="headerlink" title="1-3 监督学习"></a>1-3 监督学习</h2><ul>
<li>包括线性回归和逻辑回归</li>
</ul>
<h2 id="1-4-无监督学习"><a href="#1-4-无监督学习" class="headerlink" title="1-4 无监督学习"></a>1-4 无监督学习</h2><ul>
<li>无监督学习不需要给数据打上标签，也就是不需要人告诉机器一部分正确答案是什么</li>
</ul>
<h1 id="2-单变量线性回归"><a href="#2-单变量线性回归" class="headerlink" title="2 单变量线性回归"></a>2 单变量线性回归</h1><h2 id="2-2-代价函数"><a href="#2-2-代价函数" class="headerlink" title="2-2 代价函数"></a>2-2 代价函数</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/4c6ff4b4ec65c5f5bcc5f2ba1c4df4bc_1745678189473.png" alt="在这里插入图片描述"></p>
<ul>
<li>Hypothesis即假定函数，是线性回归时机器推测出来的对已经给定的一堆离散点进行拟合之后的函数</li>
<li>θ<del>0</del>和θ<del>1</del>是待求的参数，求出这两个参数后，就能得出一条直线进行拟合</li>
<li>Cost Function即代价函数，是对数据集中每个点与假定函数进行作差再平方再相加后求得的平均值再乘$\frac{1}{2}$得到的，乘$\frac{1}{2}$只是为了之后求导方便，式中m为数据集中数据个数共有m个，$x^{(i)}$指的是数据集第i行的x的值，$y^{(i)}$指的是数据集第i行的y的值，y即为需要通过机器学习推测的值</li>
<li>目标是求代价函数的最小值（并求出此时θ<del>0</del>和θ<del>1</del>的值），因为在代价函数的值最小时，假定函数距离各离散点的距离最近<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/2dccdadf16de6ffbb9f29a45c952fa77_1745678189473.png" alt="在这里插入图片描述"><br>由于有两个参数θ<del>0</del>和θ<del>1</del>，所以得出的代价函数图形是一个如上图所示的3D曲面图<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/1c0e39491e8b432f3a1100195ef27ffb_1745678189473.png" alt="在这里插入图片描述"><br>3D曲面图也可以用上图的等高线图表示</li>
</ul>
<h2 id="2-5-Batch-梯度下降算法"><a href="#2-5-Batch-梯度下降算法" class="headerlink" title="2-5 Batch 梯度下降算法"></a>2-5 Batch 梯度下降算法</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/7abffd8389c77105f06d2ca062295a21_1745678197454.png" alt="在这里插入图片描述"><br>梯度下降算法用于如上图的函数：在图上取一个点，向四周寻找最快的下山方向，并迈出一步，重复执行上述步骤，就可以找到代价函数的最小值（上图仅用于解释梯度下降算法，线性回归的代价函数永远是一个凸函数，只会存在一个全局最优解，不会像上图一样出现局部最优解和全局最优解）<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/ee6e1095d965c1e106a307e6e23c068d_1745678197454.png" alt="在这里插入图片描述"><br>在这里<code>:=</code>是赋值符号，<code>=</code>是两边等价的意思<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/b97f62704b9c2c1586f7ad8ecfce467d_1745678197454.png" alt="在这里插入图片描述"><br>如上图，θ<del>0</del>和θ<del>1</del>需要同步更新来实现梯度下降，α是学习率，α越大，相当于下山的步子越大，下山就越快</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/514fe6190a25fb4a672725f6334fccdd_1745678197454.png" alt="在这里插入图片描述"><br>如上图，θ<del>1</del>从右侧较大时开始逐渐向左侧递减，α是常数不改变，代价函数的导数项（这个导数项其实是偏导数）会随着代价函数逐渐降低而变小（因为斜率变小了），在到达最小值时，导数会变为0</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/643dc59253299883ba90d05f0c63a699_1745678197454.png" alt="在这里插入图片描述"><br>将梯度下降式子里的导数项求出后得到上面的式子</p>
<h1 id="4-多变量线性回归"><a href="#4-多变量线性回归" class="headerlink" title="4 多变量线性回归"></a>4 多变量线性回归</h1><h2 id="4-1-多特征"><a href="#4-1-多特征" class="headerlink" title="4-1 多特征"></a>4-1 多特征</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/423121e75a8d8c194df340aa5b539c6a_1745678205653.png" alt="在这里插入图片描述"></p>
<ul>
<li>用n来表示数据集中特征的数量，这里有4个特征</li>
<li>y表示输出量</li>
<li>用$x^{(i)}$来表示第i行的数据（输入量）</li>
<li>用$x^{(i)}_j$来表示第i行的第j个特征</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/30b208a632d7f234f1baa7d1cbed43af_1745678205653.png" alt="在这里插入图片描述"><br>上图是新的假定函数</p>
<ul>
<li>$x_1，x_2，x_3……$是多个特征</li>
<li>可以假定一个$x^{(i)}_0&#x3D;1$，这样x和θ就可以写成两个列向量，假定函数就可以写作$θ^Tx$，即求两个矩阵的内积</li>
</ul>
<h2 id="4-2-多元梯度下降法"><a href="#4-2-多元梯度下降法" class="headerlink" title="4-2 多元梯度下降法"></a>4-2 多元梯度下降法</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/3033878776917b5d410622e3fac110f8_1745678205653.png" alt="在这里插入图片描述"></p>
<ul>
<li>如上图，从$θ_1$到$θ_n$同步更新，同时特征$x$需要对应</li>
</ul>
<h2 id="4-3-多元梯度下降法I——特征缩放"><a href="#4-3-多元梯度下降法I——特征缩放" class="headerlink" title="4-3 多元梯度下降法I——特征缩放"></a>4-3 多元梯度下降法I——特征缩放</h2><h3 id="缩放"><a href="#缩放" class="headerlink" title="缩放"></a>缩放</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/b11e116c298e9c8e8eee5d456ab6ddd0_1745678205653.png" alt="在这里插入图片描述"></p>
<ul>
<li>这里假设只有两个特征值，特征$x_1$的取值范围是0-2000，特征$x_2$的取值范围是1-5，之后作出的代价函数的等高线图会是一个又高又瘦的椭圆，在进行梯度下降算法时，可能会反复震荡导致收敛太慢，如上图左侧</li>
<li>将特征$x_1$和$x_2$缩放，使这两个特征值的范围都在0-1，这样产生的代价函数的图像会变成向上图右侧一样的圆形，这样就会更快地收敛</li>
<li>一般会让特征值处于$[-1,1]$范围内，如果比较接近$\pm1$也可以直接计算，不需要进行特征缩放</li>
<li>如果特征值过小，如$[-0.0001,0.0001]$也需要进行缩放</li>
</ul>
<h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/05f5563838da819ab656573ad8921993_1745678205653.png" alt="在这里插入图片描述"></p>
<ul>
<li>如特征$x_1$的平均值为1000，范围为$(0,2000]$，则可以将$x_1$化为$x_1&#x3D;\frac{size-1000}{2000}$（在这里的例子里$x_1$是房屋的面积）</li>
<li>公式为$x_1&#x3D;\frac{x_1-μ_1}{s_1}$，$s_1$为特征$x_1$原来的范围大小（$max-min$），$μ_1$为特征$x_1$原来的平均值</li>
</ul>
<h2 id="4-4-多元梯度下降法演练I——学习率α"><a href="#4-4-多元梯度下降法演练I——学习率α" class="headerlink" title="4-4 多元梯度下降法演练I——学习率α"></a>4-4 多元梯度下降法演练I——学习率α</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/391d755b67e35a0917e4dc223440bbd4_1745678214535.png" alt="在这里插入图片描述"><br>上图左侧纵坐标为计算得出的代价函数的值，横坐标为进行梯度下降算法的次数，次数增加后，代价函数会越来越接近最小值，逐渐收敛<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/fcb1956be8bd1306e3fc2d76171d5bc9_1745678214535.png" alt="在这里插入图片描述"></p>
<ul>
<li>如果图像为上图左侧的两种，说明学习率过大，导致像上图右侧一样的变化发散</li>
<li>只要学习率足够小，一定会是收敛的（即进行梯度下降算法的次数越多，代价函数一定会越来越接近最小值）</li>
<li>学习率过小会导致收敛速度变慢</li>
</ul>
<p>取学习率时，通常是每隔大约三倍取，如$0.001,0.003,0.01,0.03.0.1,0.3,1$</p>
<h2 id="4-5-特征和多项式回归"><a href="#4-5-特征和多项式回归" class="headerlink" title="4-5 特征和多项式回归"></a>4-5 特征和多项式回归</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/9eaf6067ee96755af8bf8e0fd7c8d907_1745678214535.png" alt="在这里插入图片描述"></p>
<ul>
<li>用多次的函数来拟合，依然使用之前的一次式，但让$x_1&#x3D;size$，$x_2&#x3D;size^2$，$x_3&#x3D;size^3$</li>
<li>或者也可以令假定函数是$h_θ(x)&#x3D;θ_0+θ_1x_1+θ_2x_2&#x3D;θ_0+θ_1(size)+θ_2\sqrt{size}$</li>
</ul>
<h2 id="4-6-正规方程（Normal-Equation）（区别于迭代方法的直接解法）"><a href="#4-6-正规方程（Normal-Equation）（区别于迭代方法的直接解法）" class="headerlink" title="4-6 正规方程（Normal Equation）（区别于迭代方法的直接解法）"></a>4-6 正规方程（Normal Equation）（区别于迭代方法的直接解法）</h2><p>（<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38128785">最小二乘法</a>）<br>$$θ&#x3D;(X^TX)^{-1}X^Ty$$</p>
<ul>
<li>使用这个式子不需要进行特征缩放</li>
<li>$X$为$m×n$的矩阵，$m$为数据数量（training examples），$n$为特征个数</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/aa148678ae8aee4c9462c18a38901be2_1745678214535.png" alt="在这里插入图片描述"><br>上图是梯度下降算法和正规方程的优缺点</p>
<ul>
<li>在特征数量$n$大于10000左右时，开始考虑不使用正规方程而使用梯度下降算法</li>
</ul>
<h1 id="6-逻辑回归"><a href="#6-逻辑回归" class="headerlink" title="6 逻辑回归"></a>6 逻辑回归</h1><h2 id="6-1-逻辑分类算法"><a href="#6-1-逻辑分类算法" class="headerlink" title="6-1 逻辑分类算法"></a>6-1 逻辑分类算法</h2><ul>
<li>用于输出量$y$是离散值如0&#x2F;1的情况</li>
<li>不宜用线性回归进行机器学习</li>
</ul>
<h2 id="6-2-假设表示"><a href="#6-2-假设表示" class="headerlink" title="6-2 假设表示"></a>6-2 假设表示</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/4722d19fd65ab9affa500135b9ddfb6d_1745678214535.png" alt="在这里插入图片描述"></p>
<ul>
<li>在逻辑分类中，将假定函数改为$h_θ(x)&#x3D;g(θ^Tx)$，假定函数函数的值即为输出真（1）的概率</li>
<li>函数$g(z)$称作Sigmoid函数或Logistic函数，其表达式为$\frac{1}{1+e^{-z}}$</li>
<li>综上，逻辑分类中的假定函数为$\frac{1}{1+e^{-θ^Tx}}$</li>
</ul>
<h2 id="6-3-决策边界"><a href="#6-3-决策边界" class="headerlink" title="6-3 决策边界"></a>6-3 决策边界</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/7a63282dc8283d87c06af05d744838ab_1745678223476.png" alt="在这里插入图片描述"></p>
<ul>
<li>predict “y&#x3D;1” if $h_θ(x)\ge0.5$</li>
<li>predict “y&#x3D;0” if $h_θ(x)＜0.5$</li>
<li>在上图右上角$g(z)&#x3D;\frac{1}{1+e^{-z}}$函数的图像中，用$g(z)$来表示假定函数，要计算何时$g(z)&gt;0.5$，只要计算何时$z\ge0$，即计算何时$θ^Tx&gt;0$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/e5550041aef0e738e044836b8585b736_1745678223476.png" alt="在这里插入图片描述"></li>
<li>计算得出的决策边界可以是直线<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/639c701e430fc7bf5fdd0ef072cce42f_1745678223476.png" alt="在这里插入图片描述"></li>
<li>决策边界也可以是圆形的</li>
</ul>
<h2 id="6-4-代价函数"><a href="#6-4-代价函数" class="headerlink" title="6-4 代价函数"></a>6-4 代价函数</h2><p>之前的线性表示的代价函数为：<br>$$J(θ)&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^m\frac{1}{2}(h_θ(x^{(i)})-y^{(i)})^2$$<br>将$\frac{1}{2}(h_θ(x^{(i)})-y^{(i)})^2$表示为$Cost(h_θ(x^{(i)}),y^{(i)})$<br>去掉$(i)$之后简写为<br>$$J(θ)&#x3D;\frac{1}{2m}\sum_{i&#x3D;1}^mCost(h_θ(x),y)$$</p>
<ul>
<li>由于在逻辑回归上继续使用这个代价函数会导致输出的代价函数不是凸函数，不能使用梯度下降算法输出全局最小值</li>
<li>所以在逻辑回归中将代价函数中的$Cost(h_θ(x),y)$改为如下<br>$$Cost(h_θ(x),y)&#x3D;<br>\begin{cases}<br>-log(h_θ(x))&amp; \text{if y&#x3D;1}\<br>-log(1-h_θ(x))&amp; \text{if y&#x3D;0}<br>\end{cases}$$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/95b0e191332f0155bd400ced2b66536f_1745678223476.png" alt="在这里插入图片描述"></li>
<li>$Cost(h_θ(x),y)$中第一个函数的图像如上图</li>
<li>输出量$y&#x3D;1$时，若假定函数$h_θ(x)&#x3D;1$，那么$Cost(h_θ(x),y)$输出值将会趋向于0，即代价函数趋向于0，使得这点对假定函数的惩罚为0，几乎不改变结果</li>
<li>输出量$y&#x3D;1$时，若假定函数$h_θ(x)\rightarrow0$，那么$Cost(h_θ(x),y)$输出值将会趋向于$+\infty$，即代价函数趋向于$+\infty$，使得这点对假定函数的惩罚为$+\infty$，极大地改变结果，使假定函数在这点处的输出结果迅速回到正确的值上来</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/a6319ba45ae698b153e7b5b5e62b40b3_1745678223476.png" alt="在这里插入图片描述"></p>
<ul>
<li>$Cost(h_θ(x),y)$中第二个函数的图像如上图</li>
</ul>
<h2 id="6-4-简化代价函数与梯度下降"><a href="#6-4-简化代价函数与梯度下降" class="headerlink" title="6-4 简化代价函数与梯度下降"></a>6-4 简化代价函数与梯度下降</h2><p>由于$y&#x3D;0$ or $1$恒成立（$y&#x3D;0$ or $1$ always），所以可以将$Cost(h_θ(x),y)$简化为一个式子</p>
<p>$$Cost(h_θ(x),y)&#x3D;-ylog(h_θ(x))-(1-y)log(1-h_θ(x))$$<br>最后得出的用于逻辑回归的代价函数为：<br>$$J(θ)&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^mCost(h_θ(x^{(i)}),y^{(i)})$$<br>$$J(θ)&#x3D;-\frac{1}{m}\left[\sum_{i&#x3D;1}^my^{(i)}log(h_θ(x^{(i)}))+(1-y^{(i)})log(1-h_θ(x^{(i)}))\right]$$<br>在梯度下降中不断循环(for j&#x3D;1,2,…,n)的式子为：<br>$$ θ_j:&#x3D;θ_j-α\frac{\partial}{\partial θ_j}J(θ) $$<br>$$ θ_j:&#x3D;θ_j-α\sum_{i&#x3D;1}^m(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}$$</p>
<h2 id="6-5-高级优化"><a href="#6-5-高级优化" class="headerlink" title="6-5 高级优化"></a>6-5 高级优化</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/ce35d902d413c87ac48aeeaf9dc93c15_1745678231545.png" alt="在这里插入图片描述"></p>
<h2 id="6-7-多元分类：一对多"><a href="#6-7-多元分类：一对多" class="headerlink" title="6-7 多元分类：一对多"></a>6-7 多元分类：一对多</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/11893bf6dc86a4e11298c85e0e0bb8f8_1745678231545.png" alt="在这里插入图片描述"></p>
<ul>
<li>共有三个分类：三角形、叉、正方形</li>
<li>先计算三角形：将三角形认为是正样本，其他认为是负样本，可以计算出一条直线划分三角形和其他图形，如上图右上角第一幅坐标系</li>
<li>其他图形同理</li>
</ul>
<h1 id="7-正则化"><a href="#7-正则化" class="headerlink" title="7 正则化"></a>7 正则化</h1><h2 id="7-1-过拟合问题"><a href="#7-1-过拟合问题" class="headerlink" title="7-1 过拟合问题"></a>7-1 过拟合问题</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/818521cd2f4d5fc369bcccef02ef906d_1745678231545.png" alt="在这里插入图片描述"></p>
<ul>
<li>上图左侧坐标系为欠拟合，用一条直线不能很好的表示这个数据集，偏差很大</li>
<li>上图中间为合适的拟合</li>
<li>上图右侧为过度拟合，拟合的曲线波动很大，假定函数中的变量过多，虽然代价函数非常接近0，但不能泛化到其他数据集中<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/6a72f12479f1ecf951cd913253590e71_1745678231545.png" alt="在这里插入图片描述"></li>
<li>上图为在逻辑回归中的欠拟合、合适的拟合、过度拟合</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/fcfe1ae52d84864b8b7156043d39b228_1745678231545.png" alt="在这里插入图片描述"><br>解决过拟合：<br>1.减少特征数量<br>2.正则化</p>
<h2 id="7-2-正则化更改-代价函数"><a href="#7-2-正则化更改-代价函数" class="headerlink" title="7-2 (正则化更改)代价函数"></a>7-2 (正则化更改)代价函数</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/4a0ad386d553813fcb9a294a1002b80f_1745678240670.png" alt="在这里插入图片描述"><br>使用正则化更改代价函数</p>
<ul>
<li>直接使用上图右侧的假定函数及其代价函数会导致过拟合</li>
<li>在不去掉$θ_3$和$θ_4$的前提下，可以在代价函数上加上对于$θ_3$和$θ_4$的惩罚项：$1000θ_3^2+1000θ_3^2$，（1000只是随便一个比较大的数），加上惩罚项之后的代价函数在运算中会让参数$θ_3$和$θ_4$变得尽可能小，让$θ_3$和$θ_4$对假定函数的图像影响变小，这样就可以在保留$θ_3$和$θ_4$参数的情况下不产生过拟合。</li>
</ul>
<p>加上惩罚项后的假定函数变为：<br>$$J(θ)&#x3D;\frac{1}{2m}<br>\left[<br>\sum_{i&#x3D;1}^mCost(h_θ(x^{(i)}),y^{(i)})+1000θ_3^2+1000θ_3^2<br>\right]$$</p>
<p>由于我们一般不知道哪一项会导致过拟合，所以在代价函数中加入正则化项<br>加入后的代价函数为：<br>$$J(θ)&#x3D;\frac{1}{2m}<br>\left[<br>\sum_{i&#x3D;1}^mCost(h_θ(x^{(i)}),y^{(i)})+λ\sum_{j&#x3D;1}^{n}θ_j^2<br>\right]$$<br>$λ$为正则化参数，一般不对$θ_0$增加惩罚项，所以$j$从$1$开始</p>
<h2 id="7-3-线性回归的正则化"><a href="#7-3-线性回归的正则化" class="headerlink" title="7-3 线性回归的正则化"></a>7-3 线性回归的正则化</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/2985f048284a8192d5478c3bb8c4e3fc_1745678240670.png" alt="在这里插入图片描述"></p>
<ul>
<li>正则化后的梯度下降循环项如上图，由于在代价函数中没有添加对$θ_0$的惩罚项，所以对$θ_0$的更新分开表示<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/dd2f2b9ff3b5c83ba822abde14257762_1745678240670.png" alt="在这里插入图片描述"></li>
<li>对于$j&#x3D;1,2,3,…,n$的梯度下降项也可化简为上图的式子</li>
<li>可以得到$1-α\frac{λ}{m}$是一个小于1但非常接近1的数</li>
</ul>
<p>使用正则化后得到的正规方程为：<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/8aaae8dd41820d5689a98b00ea4e4f1f_1745678240670.png" alt="在这里插入图片描述"></p>
<ul>
<li>这里只要$λ&gt;0$，那么括号内计算得出的矩阵一定可逆</li>
<li>而在原来没有用正则化得出的正规矩阵$θ&#x3D;(X^TX)^{-1}X^Ty$中，如果$m&lt;n$（m为样本数，n为特征数），那么括号内矩阵不可逆（是奇异矩阵），所以正则化也可以应用于样本数小于特征数时的情况，让括号内矩阵可逆</li>
</ul>
<h2 id="7-4-逻辑回归的正则化"><a href="#7-4-逻辑回归的正则化" class="headerlink" title="7-4 逻辑回归的正则化"></a>7-4 逻辑回归的正则化</h2><p>正则化后逻辑回归的代价函数改为：<br>$$J(θ)&#x3D;-\frac{1}{m}\left[\sum_{i&#x3D;1}^my^{(i)}log(h_θ(x^{(i)}))+(1-y^{(i)})log(1-h_θ(x^{(i)}))\right]+\frac{λ}{2m}\sum_{j&#x3D;1}^{n}θ_j^2$$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/dd067a89502c182f94d9e40a6978a120_1745678240670.png" alt="在这里插入图片描述"><br>正则化后的梯度下降循环项如上图</p>
<h1 id="8-神经网络的表示"><a href="#8-神经网络的表示" class="headerlink" title="8 神经网络的表示"></a>8 神经网络的表示</h1><h2 id="8-1-非线性假设"><a href="#8-1-非线性假设" class="headerlink" title="8-1 非线性假设"></a>8-1 非线性假设</h2><p>对于一幅图像来说，如果取出每个像素点的灰度值或其他特征表示方法来作为一个数据样本的话，数据集将会非常庞大，如果运用之前的回归算法来计算，将会产生非常大的计算成本</p>
<h2 id="8-2-神经元与大脑"><a href="#8-2-神经元与大脑" class="headerlink" title="8-2 神经元与大脑"></a>8-2 神经元与大脑</h2><h2 id="8-3-前向传播-模型展示I"><a href="#8-3-前向传播-模型展示I" class="headerlink" title="8-3 前向传播-模型展示I"></a>8-3 前向传播-模型展示I</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/0256736d30f13e4463516fa990ccff94_1745678248804.png" alt="在这里插入图片描述"></p>
<ul>
<li>上图指一个带有Sigmoid激活函数的人工神经元，在神经网络术语中，$g(z)&#x3D;\frac{1}{1+e^{-θ^TX}}$称作激活函数<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/2cf42670a869dd7c50cfdb447f4bd662_1745678248804.png" alt="在这里插入图片描述"></li>
<li>神经网络指的其实是一组神经网络的集合，第一层（Layer 1）称为输入层（Input Layer），第二层（Layer 2）称为隐藏层（Hidden Layer），第三层（Layer 3）称为输出层（Output Layer）</li>
<li>用$a_i^{(j)}$来表示第$j$层的第$i$个神经元的激活项（”activation” of unit $i$ in layer $j$），所谓激活项是指由一个具体的神经元计算并输出的值</li>
<li>用$\Theta^{(j)}$表示从第$j$层到第$j+1$层的权重矩阵（参数矩阵），就是之前的$\theta$矩阵（之前的$\theta$既可以叫做参数$parameters$也可以叫做权重$weights$）<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/181dd6e65c63318fe61632dcd42d9312_1745678248804.png" alt="在这里插入图片描述"></li>
<li>$a_1^{(2)}$、$a_2^{(2)}$和$a_3^{(2)}$的计算公式在上图中已写出</li>
<li>其中$\Theta^{(1)}$是一个$3×4$的矩阵</li>
<li>如果神经网络在第$j$层有$s_j$个单元，在第$j+1$层有$s_{j+1}$个单元，那么$\Theta^{(j)}$是一个$s_{j+1}×(s_j+1)$的矩阵</li>
</ul>
<h2 id="8-4-前向传播-模型展示II"><a href="#8-4-前向传播-模型展示II" class="headerlink" title="8-4 前向传播-模型展示II"></a>8-4 前向传播-模型展示II</h2><p>前向传播的矢量化实现：<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/d371623e89f04948b75594786cbff9f6_1745678248804.png" alt="在这里插入图片描述"></p>
<ul>
<li>把上面式子中的$\Theta^{(1)}<em>{10}+\Theta^{(1)}</em>{11}+\Theta^{(1)}<em>{12}+\Theta^{(1)}</em>{13}$表示为$z_1^{(2)}$</li>
<li>则$a_1^{(2)}&#x3D;g(z_1^{(2)})$</li>
<li>扩展到全域，第二层的激活值$a^{(2)}&#x3D;g(z^{(2)})$，其中$z^{(2)}&#x3D;\Theta^{(1)}a^{(1)}$，另外需要添加偏置项$a^{(2)}_0&#x3D;1$</li>
</ul>
<h2 id="8-5-例子与理解I"><a href="#8-5-例子与理解I" class="headerlink" title="8-5 例子与理解I"></a>8-5 例子与理解I</h2><h2 id="8-6-例子与理解II"><a href="#8-6-例子与理解II" class="headerlink" title="8-6 例子与理解II"></a>8-6 例子与理解II</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/7fbdea27e2d61d432579e73ea6539abe_1745678248804.png" alt="在这里插入图片描述"><br>上图为计算$x_1$ XNOR $x_2$的神经网络<br>第一层到第二层先计算$x_1$ AND $x_2$得到$a_1^{(2)}$，计算(NOT $x_1$) AND (NOT $x_2$)得到$a_2^{(2)}$<br>再以$a_1^{(2)}$和$a_2^{(2)}$为$x_1$和$x_2$计算$x_1$ OR $x_2$得出的结果即为$x_1$ XNOR $x_2$</p>
<h2 id="8-7-多元分类"><a href="#8-7-多元分类" class="headerlink" title="8-7 多元分类"></a>8-7 多元分类</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/2140fb22ca0b1d4edbad54f1f53a3746_1745678258356.png" alt="在这里插入图片描述"><br>这里有四种输出：pedestrian、car、motorcycle、truck<br>所以共有四个输出单元<br>输出的$y^{(i)}$为一个4维矩阵，可能是：<br>$$\begin{bmatrix}<br>1\<br>0\<br>0\<br>0\<br>\end{bmatrix}或<br>\begin{bmatrix}<br>0\<br>1\<br>0\<br>0\<br>\end{bmatrix}或<br>\begin{bmatrix}<br>0\<br>0\<br>1\<br>0\<br>\end{bmatrix}或<br>\begin{bmatrix}<br>0\<br>0\<br>0\<br>1\<br>\end{bmatrix}中的其中一个$$<br>分别表示pedestrian或car或motorcycle或truck</p>
<h1 id="9-神经网络：Learning"><a href="#9-神经网络：Learning" class="headerlink" title="9 神经网络：Learning"></a>9 神经网络：Learning</h1><h2 id="9-1-应用于神经网络的代价函数"><a href="#9-1-应用于神经网络的代价函数" class="headerlink" title="9-1 应用于神经网络的代价函数"></a>9-1 应用于神经网络的代价函数</h2><ul>
<li>用$L$表示神经网络的总层数（Layers）</li>
<li>用$s_l$表示第$l$层单元（神经元）的数量（不包括偏置单元）</li>
<li>$h_\Theta(x)\in\mathbb{R}^K$（$h_\Theta(x)$为$K$维向量，即神经网络输出层共有$K$个神经元，即有$K$个输出）</li>
<li>$(h_\Theta(x))<em>i&#x3D;i^{th} output$（$(h</em>\Theta(x))_i$表示第$i$个输出）</li>
</ul>
<p>应用于神经网络的代价函数为：</p>
<p>$$J(\Theta)&#x3D;-\frac{1}{m}\left[\sum_{i&#x3D;1}^m\sum_{k&#x3D;1}^Ky^{(i)}log(h_\Theta(x^{(i)}))<em>k+(1-y_k^{(i)})log(1-(h</em>\Theta(x^{(i)}))<em>k)\right]<br>+\frac{λ}{2m}\sum</em>{l&#x3D;1}^{L-1}\sum_{i&#x3D;1}^{s_l}\sum_{j&#x3D;1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2$$</p>
<ul>
<li>第二项中的$\sum_{i&#x3D;1}^{s_l}\sum_{j&#x3D;1}^{s_{l+1}}$指的是将$s_{l+1}$行$s_l$列的矩阵$\Theta_{ji}^{(l)}$中的每一个元素相加起来</li>
<li>第二项中的$\sum_{l&#x3D;1}^{L-1}$指的是将输入层和隐藏层的矩阵都求和</li>
</ul>
<h2 id="9-2-反向传播算法"><a href="#9-2-反向传播算法" class="headerlink" title="9-2 反向传播算法"></a>9-2 反向传播算法</h2><ul>
<li>$\delta_j^{(l)}$定义为第$l$层第$j$个神经元的偏差（”error”）<br> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/cbc7bcf7cea7c1ca40e449dbbbc4d9db_1745678258356.png" alt="在这里插入图片描述"><br>以上图的四层的神经网络为例</li>
<li>$\delta_j^{(4)}&#x3D;a_j^{(4)}-y_j$（$y_j$指第$j$个输出在数据集中的值，$a_j^{(4)}$指神经网络的第$j$个输出，$a_j^{(4)}$也可表示为$(h_\Theta(x))_j$）</li>
<li>用向量方法表示上式可表示为$\delta^{(4)}&#x3D;a^{(4)}-y$，也可表示为$\delta^{(4)}&#x3D;h_\Theta(x)-y$</li>
<li>$\delta^{(3)}&#x3D;(\Theta^{(3)})^T\delta^{(4)}\cdot g^{\prime}(z^{(3)})$<br>其中$g^{\prime}(z^{(3)})&#x3D;a^{(3)}\cdot (1-a^{(3)})$</li>
<li>$\delta^{(2)}&#x3D;(\Theta^{(2)})^T\delta^{(3)}\cdot g^{\prime}(z^{(2)})$<br>其中$g^{\prime}(z^{(2)})&#x3D;a^{(2)}\cdot (1-a^{(2)})$</li>
</ul>
<p><em>点乘结果是一个数，叉乘结果是一个向量</em></p>
<ul>
<li>$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)&#x3D;a_j^{(l)}\delta_i^{(l+1)}$<br>这里忽略了正则化项，即认为$\lambda&#x3D;0$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/8b8dc7cd04c37e6f2caf993104f30dc1_1745678258356.png" alt="在这里插入图片描述"></li>
<li>上图是反向传播算法的流程，最后可以得到$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)&#x3D;D^{(l)}_{ij}$，然后进行梯度下降算法</li>
</ul>
<h2 id="9-3-理解反向传播"><a href="#9-3-理解反向传播" class="headerlink" title="9-3 理解反向传播"></a>9-3 理解反向传播</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/4d2d921b4be9de818a368e42f704669c_1745678258356.png" alt="在这里插入图片描述"><br>以上图的神经网络为例</p>
<ul>
<li>$\delta_2^{(2)}&#x3D;\Theta_{12}^{(2)}\delta_1^{(3)}+\Theta_{22}^{(2)}\delta_2^{(3)}$</li>
<li>$\delta_2^{(3)}&#x3D;\Theta_{12}^{(3)}\delta_1^{(4)}$</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/9148e07c049cea920bb9329f6a33f82e_1745678258356.png" alt="在这里插入图片描述"></p>
<h2 id="9-4-展开参数"><a href="#9-4-展开参数" class="headerlink" title="9-4 展开参数"></a>9-4 展开参数</h2><h2 id="9-5-梯度检测"><a href="#9-5-梯度检测" class="headerlink" title="9-5 梯度检测"></a>9-5 梯度检测</h2><p>要估计代价函数$J(\Theta)$上点$(\theta,J(\Theta))$处的导数值，可以运用$\frac{\mathrm{d} }{\mathrm{d} \theta}J(\theta)\approx\frac{J(\theta+\varepsilon)-J(\theta-\varepsilon)}{2\varepsilon}(\varepsilon&#x3D;10^{-4}为宜)$求得导数<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/1c5d8132abb9cdad93f5d743f395082d_1745678267137.png" alt="在这里插入图片描述"><br>扩展到向量中，如上图</p>
<ul>
<li>$\theta$是一个$n$维向量，是矩阵$\Theta^{(1)},\Theta^{(2)},\Theta^{(3)},…$的展开</li>
<li>可以估计$\frac{\partial}{\partial \theta_{n}}J(\theta)$的值</li>
</ul>
<p>将估计得到的偏导数值与反向传播得到的偏导数值比较，如果两个值非常近，就可以验证计算是正确的<br><em>一旦确定反向传播算法计算出的值是正确的，就应该关掉梯度检验算法</em></p>
<h2 id="9-6-随机初始化"><a href="#9-6-随机初始化" class="headerlink" title="9-6 随机初始化"></a>9-6 随机初始化</h2><p>如果在程序开始时令$\Theta$中所有元素均为0，会导致多个神经元计算相同的特征，导致冗余，这成为对称权重问题<br>所以在初始化时要令$\Theta^{(l)}_{ij}$等于$[-\epsilon,\epsilon]$中的一个随机值</p>
<h2 id="9-7-回顾总结"><a href="#9-7-回顾总结" class="headerlink" title="9-7 回顾总结"></a>9-7 回顾总结</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/0a279e8d7a56312cc10bf2bf4824f3b5_1745678267137.png" alt="在这里插入图片描述"><br>训练一个神经网络：<br>1.随机一个初始权重<br>2.执行前向传播算法，得到对所有$x^{(i)}$的$h_\Theta(x^{(i)})$<br>3.计算代价函数$J(\Theta)$<br>4.执行反向传播算法，计算$\frac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$<br>(get $a^{(l)}$ and $\delta^{(l)}$ for $l&#x3D;2,…,L$)<br>5.通过梯度检验算法得到估计的$J(\Theta)$的偏导数值，将估计得到的偏导数值与反向传播得到的偏导数值比较，如果两个值非常近，就可以验证反向传播算法的计算结果是正确的；验证完后，关闭梯段检验算法(disable gradient checking code)<br>6.运用梯度下降算法或其他更高级的优化方法，结合反向传播计算结果，得到使$J(\Theta)$最小时的参数$\Theta$的值</p>
<h1 id="10-应用机器学习的建议"><a href="#10-应用机器学习的建议" class="headerlink" title="10 应用机器学习的建议"></a>10 应用机器学习的建议</h1><h2 id="10-1-决定下一步做什么"><a href="#10-1-决定下一步做什么" class="headerlink" title="10-1 决定下一步做什么"></a>10-1 决定下一步做什么</h2><h2 id="10-2-评估假设函数"><a href="#10-2-评估假设函数" class="headerlink" title="10-2 评估假设函数"></a>10-2 评估假设函数</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/08b7d8896b44ed82ecbde8b7657ce546_1745678267137.png" alt="在这里插入图片描述"><br>随机选择数据集中的70%作为训练集，30%作为测试集，将数据集分为两个部分</p>
<ul>
<li>首先根据训练集计算出参数$\Theta$</li>
<li>把参数$\Theta$代入测试集计算代价函数的值（这里是线性回归的代价函数，逻辑回归的代价函数同理）：<br>$$J_{test}(θ)&#x3D;\frac{1}{2m_{test}}\sum_{i&#x3D;1}^{m_{test}}(h_θ(x^{(i)}<em>{test})-y^{(i)}</em>{test})^2$$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/9945df6984e23f9ed0088aae01be3484_1745678267137.png" alt="在这里插入图片描述"></li>
</ul>
<p>逻辑分类中有另一种形式的测试度量，称作错误分类或0&#x2F;1错误分类，计算过程如上图</p>
<h2 id="10-3-模型选择和训练、验证、测试集"><a href="#10-3-模型选择和训练、验证、测试集" class="headerlink" title="10-3 模型选择和训练、验证、测试集"></a>10-3 模型选择和训练、验证、测试集</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/e9ae65812ca01629e3557c77618ebe49_1745678267137.png" alt="在这里插入图片描述"><br>随机选择数据集中的60%作为训练集（Training Set），20%作为交叉验证集（验证集，Cross Validation Set，cv），20%作为测试集（Test Set），将数据集分为三个部分<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/undefined/hexo/186b50bc6b176a9782b58b606ad95381_1745678276105.png" alt="在这里插入图片描述"><br>上图，计算训练误差、验证误差和测试误差<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/0ba151116034f55c12f507cd2b3543b5_1745678276105.png" alt="在这里插入图片描述"><br>用$d$来表示假定函数的多项式的最高次幂</p>
<ul>
<li>先用训练集求每一个假定函数的代价函数$J(\Theta)$取到最小值时$\Theta$的值，再把这个求得的$\Theta$代入交叉验证集求得$J_{cv}(\Theta)$，对每一个假定函数进行以上步骤的计算，得到$d&#x3D;1,…,10$的10个代价函数的值，取最小的那个，这里假定是$d&#x3D;4$，再使用$d&#x3D;4$时的$\Theta$代入测试集得到泛化误差</li>
</ul>
<h2 id="10-4-判断偏差与方差"><a href="#10-4-判断偏差与方差" class="headerlink" title="10-4 判断偏差与方差"></a>10-4 判断偏差与方差</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/80d269a25ed13433512ae9dbd721cdca_1745678276105.png" alt="在这里插入图片描述"><br>上图坐标系$y$轴为误差，$x$轴为$d$（多项式最高次幂）的大小<br>以上图为例，</p>
<ul>
<li>当$d$过小时，出现欠拟合(underfit)，偏差(bias)过大，此时的训练集误差$J_{train}(\Theta)$很大，并且$J_{cv}(\Theta)\approx J_{train}(\Theta)$</li>
<li>当$d$过大时，出现过拟合(overfit)，方差(variance)过大，此时的训练集误差$J_{train}(\Theta)$很小，并且$J_{cv}(\Theta)&gt;&gt; J_{train}(\Theta)$</li>
</ul>
<h2 id="10-5-正则化和偏差、方差"><a href="#10-5-正则化和偏差、方差" class="headerlink" title="10-5 正则化和偏差、方差"></a>10-5 正则化和偏差、方差</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/f15e27f7dc462c0fe3afe902d4ae7169_1745678276105.png" alt="在这里插入图片描述"><br><em>勘误：正则化项里面$\sum$上面的m应该改为n</em></p>
<ul>
<li>若正则化参数$\lambda$过大，会导致欠拟合，高偏差，并且参数$\theta_1\approx 0,\theta_2\approx 0,…$，假定函数的结果$h_{\theta}(x)\approx \theta_0$，如上图最左侧坐标系</li>
<li>若正则化参数$\lambda$过小，会导致过拟合，高方差，如上图最右侧坐标系<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/undefined/hexo/186b50bc6b176a9782b58b606ad95381_1745678276105.png" alt="在这里插入图片描述"><br>使用不带正则化项的$J_{train}(\Theta)、J_{cv}(\Theta)、J_{test}(\Theta)$<br>$J(\Theta)$取下图中的带正则化项的代价函数<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/9ccd43b5fbf97b4fe5295a0d1e13e32b_1745678284730.png" alt="在这里插入图片描述"></li>
<li>首先取多种$\lambda$的情况，这里从0,0.01开始，下一个$\lambda$是上一个的两倍，一直取到$\lambda&#x3D;10.24$，由于此时小数部分对结果影响不大，所以也可以直接取$\lambda&#x3D;10$</li>
<li>将每一种情况的$\lambda$代入带正则化项的代价函数$J(\Theta)$中，这里共能得到12个不同的$\Theta$值</li>
<li>再把这12个$\Theta$代入到交叉验证集代价函数$J_{cv}(\Theta)$中，注意这里的$J_{cv}(\Theta)$是不带正则化项的，最后得到12个不同的$J_{cv}(\Theta)$值</li>
<li>取最小的那个$J_{cv}(\Theta)$值对应的$\lambda$（这里认为第5个$\lambda$是最合适的）代入到测试集的代价函数$J_{test}(\Theta)$中，计算泛化误差，注意这里的$J_{test}(\Theta)$也是不带正则化项的<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/d5c5aed8bf1070c5af79ba24641f4238_1745678284730.png" alt="在这里插入图片描述"></li>
</ul>
<h2 id="10-6-学习曲线"><a href="#10-6-学习曲线" class="headerlink" title="10-6 学习曲线"></a>10-6 学习曲线</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/5666a151a1cec7849f7fb7e89547a5be_1745678284730.png" alt="在这里插入图片描述"><br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/dddb053dcf4b7484713f56ed29710f1b_1745678284730.png" alt="在这里插入图片描述"><br>如上图是高偏差&#x2F;欠拟合的情形<br>此时增加数据集数量对于误差的缩小没有明显帮助<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/545cdba04bc4ae7ed52f23a5dc3f5dec_1745678284730.png" alt="在这里插入图片描述"><br>如上图是高方差&#x2F;过拟合的情形<br>此时交叉验证集的曲线和训练集的曲线中间相差较大，所以增加数据集数量对于减小误差是有帮助的</p>
<h2 id="10-7-决定接下来做什么"><a href="#10-7-决定接下来做什么" class="headerlink" title="10-7 决定接下来做什么"></a>10-7 决定接下来做什么</h2><p>解决高偏差或高方差的一些方法如下图：<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/f3e53ecc8259212e5859d664f66ef2c8_1745678294895.png" alt="在这里插入图片描述"><br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/97a1b4f9a4843ea536534ea807a2265b_1745678294895.png" alt="在这里插入图片描述"><br>如上图</p>
<ul>
<li>在一个小型的神经网络中，容易出现欠拟合现象，但它的好处是计算量较小</li>
<li>在一个大型的神经网络中，容易出现过拟合现象，可以通过正则化来避免，它的好处是效果较好，但是计算量较大</li>
</ul>
<h1 id="11-机器学习系统设计"><a href="#11-机器学习系统设计" class="headerlink" title="11 机器学习系统设计"></a>11 机器学习系统设计</h1><h2 id="11-1-确定执行的优先级：以垃圾邮件分类为例"><a href="#11-1-确定执行的优先级：以垃圾邮件分类为例" class="headerlink" title="11-1 确定执行的优先级：以垃圾邮件分类为例"></a>11-1 确定执行的优先级：以垃圾邮件分类为例</h2><h2 id="11-2-误差分析"><a href="#11-2-误差分析" class="headerlink" title="11-2 误差分析"></a>11-2 误差分析</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/20c87a7646f6746eb448249d2074fcdc_1745678294895.png" alt="在这里插入图片描述"></p>
<ul>
<li>不要在编写程序时进行过早的优化，在编写完整个系统后再根据学习曲线优化系统；先编写一个简单粗暴的算法，再根据输出的结果改进算法</li>
<li>误差分析：观察被错误分类的数据有何共同的特征（以垃圾邮件分类为例，查看被错误分类的垃圾邮件有何共同的特征）或以数值方式表示误差</li>
<li>推荐在交叉验证集上做误差分析</li>
</ul>
<h2 id="11-3-不对称性分类的误差评估"><a href="#11-3-不对称性分类的误差评估" class="headerlink" title="11-3 不对称性分类的误差评估"></a>11-3 不对称性分类的误差评估</h2><p>偏斜类问题：一个数据集中的一类数据样本比另外一类的样本少的多（比如1占样本的0.5%，而0占样本的99.5%）<br>在偏斜类问题中（比如患癌症占样本的0.5%，而不患癌症占样本的99.5%），假设一个算法达到了99.5%的预测准确率，这时已经只有0.5%的误差，但是如果始终预测不患癌症也有99.5%的准确率，误差也只有0.5%，这时需要用到不同的误差度量值<br>其中一种叫做查准率和召回率(precision recall)<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/8d7e803f474069ce34308920c811f1bb_1745678294895.png" alt="在这里插入图片描述"></p>
<ul>
<li>左侧表格上方为实际的，表格左侧为预测的，1代表患癌，0代表不患癌<ul>
<li>如果预测为1，实际为1，则是真阳性(True positive)</li>
<li>如果预测为1，实际为0，则是假阳性(False positive)</li>
<li>如果预测为0，实际为1，则是假阴性(False negtive)</li>
<li>如果预测为0，实际为0，则是真阴性(True negtive)<br>$$查准率&#x3D;\frac{预测为真并且实际为真的数量}{预测为真的数量}&#x3D;\frac{True\ positive}{True\  positive+False\  positive}$$<br>$$召回率&#x3D;\frac{预测为真并且实际为真的数量}{实际为真的数量}&#x3D;\frac{True\ positive}{True\  positive+False\  negtive}$$<br>两者均是越高越好</li>
</ul>
</li>
</ul>
<h2 id="11-4-查准率和召回率的平衡"><a href="#11-4-查准率和召回率的平衡" class="headerlink" title="11-4 查准率和召回率的平衡"></a>11-4 查准率和召回率的平衡</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/81ea539e4656321d04f53c50b5dd01a9_1745678294895.png" alt="在这里插入图片描述"><br>本来在超过50%概率时预测为1（即患癌），在低于50%时预测为0（即不患癌）<br>现在如果要在超过70%患癌可能性时才预测为患癌的话，会得到一个高查准率但低召回率的模型<br>如果要在超过30%患癌可能性时就预测患癌，会得到一个高召回率但低查准率的模型</p>
<ul>
<li>高查准率：能不揪出来的就不揪出来</li>
<li>高召回率：能揪出来的都揪出来</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/1be4090a2e4bed36698e834a0a24697d_1745678304135.png" alt="在这里插入图片描述"><br>通过计算查准率和召回率的平均值不能直接评估一个算法的好坏<br>通过$F值（F_1值）&#x3D;2\frac{PR}{P+R}$能更好地评估一个算法的好坏（P为查准率，R为召回率）</p>
<h2 id="11-5-机器学习数据"><a href="#11-5-机器学习数据" class="headerlink" title="11-5 机器学习数据"></a>11-5 机器学习数据</h2><p> 在有一个非常庞大的训练集的前提下，即使有很多的参数（训练集数量&gt;&gt;参数数量），也能很好地拟合数据集，不会产生过拟合现象</p>
<h1 id="12-支持向量机（SVM）"><a href="#12-支持向量机（SVM）" class="headerlink" title="12 支持向量机（SVM）"></a>12 支持向量机（SVM）</h1><h2 id="12-1-优化目标"><a href="#12-1-优化目标" class="headerlink" title="12-1 优化目标"></a>12-1 优化目标</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/254f377913b6c9eaf38fbf5231682913_1745678304135.png" alt="在这里插入图片描述"></p>
<ul>
<li>上图左侧坐标系为$y&#x3D;1$时代价函数的图像，支持向量机时画出粉色的曲线，命名为$Cost_1(z)$，下标指$y$的值为$1$</li>
<li>同样的，右侧坐标系为$y&#x3D;0$时代价函数的图像，支持向量机时画出粉色的曲线，命名为$Cost_0(z)$，下标指$y$的值为$0$</li>
</ul>
<p>在逻辑回归中，代价函数为：<br>$$J(θ)&#x3D;-\frac{1}{m}\left[\sum_{i&#x3D;1}^my^{(i)}log(h_θ(x^{(i)}))+(1-y^{(i)})log(1-h_θ(x^{(i)}))\right]+\frac{λ}{2m}\sum_{j&#x3D;1}^{n}θ_j^2$$<br>要支持向量机，先将上式里的负号放到求和里面去，然后把$\frac{1}{m}$去掉（$\frac{1}{m}$是一个常数，虽然去掉它会改变代价函数的值，但是仍然能求得一样的最小值对应的$\theta$），之后得到的代价函数为：<br>$$J(θ)&#x3D;\sum_{i&#x3D;1}^m\left[y^{(i)}\left (-log(h_θ(x^{(i)}))\right )+(1-y^{(i)})\left(-log(1-h_θ(x^{(i)}))\right)\right]+\frac{λ}{2}\sum_{j&#x3D;1}^{n}θ_j^2$$<br>把上式中的$\left (-log(h_θ(x^{(i)}))\right )$替换为$Cost_1(\theta^Tx^{(i)})$，把$\left(-log(1-h_θ(x^{(i)}))\right)$替换为$Cost_0(\theta^Tx^{(i)})$，得到代价函数：<br>$$J(θ)&#x3D;\sum_{i&#x3D;1}^m\left[y^{(i)}Cost_1(\theta^Tx^{(i)})+(1-y^{(i)})Cost_0(\theta^Tx^{(i)})\right]+\frac{λ}{2}\sum_{j&#x3D;1}^{n}θ_j^2$$<br>在支持向量机中，不再使用正则化参数$\lambda$，改为使用参数$C$，更改后的支持向量机的代价函数为：<br>$$J(θ)&#x3D;C\sum_{i&#x3D;1}^m\left[y^{(i)}Cost_1(\theta^Tx^{(i)})+(1-y^{(i)})Cost_0(\theta^Tx^{(i)})\right]+\frac{1}{2}\sum_{j&#x3D;1}^{n}θ_j^2$$</p>
<ul>
<li>支持向量机中不会预测$y&#x3D;1&#x2F;0$的概率，如果$\theta^Tx^{(i)}\ge0$，假定函数输出1，反之输出0</li>
</ul>
<h2 id="12-2-大间距的理解"><a href="#12-2-大间距的理解" class="headerlink" title="12-2 大间距的理解"></a>12-2 大间距的理解</h2><p>支持向量机又被称为大间距分类器<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/be8966d6e06e5de9524ad31e42e5c7ec_1745678304135.png" alt="在这里插入图片描述"><br>更改支持向量机中的判断边界，让$\theta^Tx^{(i)}\ge1$时才输出1，$\theta^Tx^{(i)}\le-1$时才输出0，这样两个结果之间就有一个安全间距<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/f773adbeabbe9e18837515c93f7603aa_1745678304135.png" alt="在这里插入图片描述"><br>使用一般的逻辑回归算法可能会生成上图中粉色和绿色的直线来分割两类样本，而使用支持向量机会生成图中黑色的直线，图中两条蓝线中间的区域称为间距，支持向量机会尽可能的将两种样本以最大的间距分隔开，可以看出，支持向量机后可以有更好地鲁棒性<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/dda6d32fe627169c3ecc9e413d88c3d0_1745678304135.png" alt="在这里插入图片描述"><br>如上图，先假设没有左侧的那一个负样本，用一个很大的$C$可以生成上图中黑色直线，但如果有左侧的那一个负样本，由于$C$很大，支持向量机为保证两类样本间有最大的间距，会生成上图中粉色的直线，但如果$C$没有那么大，则即使有左侧的那一个负样本，依然会生成黑色直线</p>
<ul>
<li>$C$就相当于之前的$\frac{1}{\lambda}$，虽然两者确实不一样，但呈现出来的效果差不多</li>
</ul>
<h2 id="12-3-支持向量机的数学原理"><a href="#12-3-支持向量机的数学原理" class="headerlink" title="12-3 支持向量机的数学原理"></a>12-3 支持向量机的数学原理</h2><h2 id="12-4-核函数I"><a href="#12-4-核函数I" class="headerlink" title="12-4 核函数I"></a>12-4 核函数I</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/20c196a8473fabae7a9b2cf24a93ad6c_1745678313366.png" alt="在这里插入图片描述"><br>如图的样本，如果假定函数$\ge0$，就预测$y&#x3D;1$，其他情况预测$y&#x3D;0$，<br>在上图中的假定函数中，设$f_1&#x3D;x_1,f_2&#x3D;x_2,f_3&#x3D;x_1x_2,f_4&#x3D;x_1^2,…$<br>假定函数变为$h_\theta(x)&#x3D;\theta_0+\theta_1 f_1+\theta_2 f_2+…$<br><em>以上方法与下文方法无关，与核函数无关</em><br>然而，除了对原有的特征进行组合以外，有没有更好的方法来构造𝑓1, 𝑓2, 𝑓3？我们可以利用核函数来计算出新的特征。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/59e298765fb2e887132e93160640519e_1745678313366.png" alt="在这里插入图片描述"><br>在坐标系中取3个样本标记矩阵$l^{(1)}、l^{(2)}、l^{(3)}$<br>$x$是一个给定的训练实例，假设有两个特征值，那么$x&#x3D;[x_1,x_2]$<br>令<br>$$f_1&#x3D;similarity\left(x,l^{(1)}\right)&#x3D;exp\left(-\frac{\Vert x-l^{(1)} \Vert ^2}{2\sigma^2}\right)$$<br>$$f_2&#x3D;similarity\left(x,l^{(2)}\right)&#x3D;exp\left(-\frac{\Vert x-l^{(2)} \Vert^2 }{2\sigma^2}\right)$$<br>$$f_3&#x3D;……$$<br>$$……$$</p>
<ul>
<li>$similarity\left(x,l^{(i)}\right)$称为相似度量函数&#x2F;核函数</li>
<li>$similarity\left(x,l^{(i)}\right)$也可写为$k\left(x,l^{(i)}\right)$</li>
<li>$exp(x)$表示$e^x$，这里称为高斯核函数</li>
<li>$\sigma^2$是高斯核函数的参数</li>
</ul>
<p>核函数运算时可化为：<br>$$f_1&#x3D;similarity\left(x,l^{(1)}\right)&#x3D;exp\left(-\frac{\Vert x-l^{(1)} \Vert ^2}{2\sigma^2}\right)&#x3D;exp\left(-\frac{\sum_{j&#x3D;1}^n(x_j-l_j^{(1)} )^2}{2\sigma^2}\right)$$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/526752e74f05e7779d9ff1ab77dc4cf5_1745678313366.png" alt="在这里插入图片描述"><br>如果$x$非常接近标记$l^{(1)}$，那么$f_1\approx exp(-\frac{0^2}{2\sigma^2})\approx1$<br>如果$x$远离标记$l^{(1)}$，那么$f_1\approx exp(-\frac{(large\ number)^2}{2\sigma^2})\approx0$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/348cebfcdd686a3d8a5d4c18519e10cf_1745678313366.png" alt="在这里插入图片描述"><br>上图，如果$\sigma^2$变大，那么$f_i$的下降速度会变慢（斜率降低）<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/0b689423dd6e69ff4b893c8d33feac23_1745678313366.png" alt="在这里插入图片描述"><br>如上图，取一点$x$，已经通过支持向量机计算得出$\theta_0、\theta_1、…$的值如上图，然后可以按照核函数计算出$f_0、f_1、…$的值如上图，将$\theta$和$f$的值代入假定函数得到$0.5\ge0$，所以预测$y&#x3D;1$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/375c8daf3e18d84df9561138e09fd53a_1745678322868.png" alt="在这里插入图片描述"><br>如上图，再取另外的点$x$，假设：最后得到靠近$l^{(1)}和l^{(2)}$的点会预测为1，而远离$l^{(1)}和l^{(2)}$的点会预测为0，那么最后可以拟合出一条如图中红色的曲线，曲线内预测为1，曲线外预测为0</p>
<h2 id="12-5-核函数II"><a href="#12-5-核函数II" class="headerlink" title="12-5 核函数II"></a>12-5 核函数II</h2><p>如何选择地标？<br>　　我们通常是根据训练集的数量选择地标的数量，即如果训练集中有𝑚个实例，则我们选<br>取𝑚个地标，并且令:𝑙(1) &#x3D; 𝑥(1), 𝑙(2) &#x3D; 𝑥(2), . . . . . , 𝑙(𝑚) &#x3D; 𝑥(𝑚)。这样做的好处在于：现在我们<br>得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即：<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/8e84977a4569afb0b987300f734d814f_1745678322868.png" alt="在这里插入图片描述"><br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/4450e3d0a7067f1f2e37a06b86145e5a_1745678322868.png" alt="在这里插入图片描述"><br>由<br>$f_1^{(i)}&#x3D;similarity\left(x^{(i)},l^{(1)}\right)$<br>$f_2^{(i)}&#x3D;similarity\left(x^{(i)},l^{(2)}\right)$<br>$…$<br>$f_m^{(i)}&#x3D;similarity\left(x^{(i)},l^{(m)}\right)$<br>将$f$写为特征向量形式得到<br>$$f^{(i)}&#x3D;<br>\begin{bmatrix}<br>f_0^{(i)}&#x3D;1\<br>f_1^{(i)}\<br>f_2^{(i)}\<br>…\<br>f_m^{(i)}<br>\end{bmatrix}$$<br>$f^{(i)}$是一个$m+1$维矩阵，因为除了$m$个样本外，还加入了一个偏置项$f_0^{(i)}&#x3D;1$<br>&#x3D;&#x3D;矩阵$f^{(i)}$的含义是(第$i$个样本里的所有特征)与(从1到m的每一个样本里的所有特征)进行核函数运算，一共m个运算结果排列在矩阵里，再加上第0个的$f_0^{(i)}&#x3D;1$&#x3D;&#x3D;<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/de4d9aa2792609116134d6223aadcc9d_1745678322868.png" alt="在这里插入图片描述"><br>用$f^{(i)}$替换带$x$项，得到代价函数为：<br>$$C\sum_{i&#x3D;1}^m\left[y^{(i)}Cost_1(\theta^Tf^{(i)})+(1-y^{(i)})Cost_0(\theta^Tf^{(i)})\right]+\frac{1}{2}\sum_{j&#x3D;1}^{n&#x3D;m}θ_j^2$$</p>
<ul>
<li>&#x3D;&#x3D;关于正则化项中$n&#x3D;m$的解释：$\theta_{(i)}$是对应于矩阵$f^{(i)}$的权重，由于一共有$i&#x3D;1,2,…,m$共m个$f^{(i)}$对应着m个$\theta$，所以假定函数中一共有m项，即有m个$\theta$，而之前已经规定了权重$\theta$的个数用n来表示，所以这里有$n&#x3D;m$&#x3D;&#x3D; 注意这里已经忽略了$\theta_0$，不对其进行正则化</li>
</ul>
<p>正则化项在具体实施中，求和部分可写为$\sum_{j&#x3D;1}^{n&#x3D;m}θ_j^2&#x3D;\theta^T\theta$，并且在计算时用$\theta^TM\theta$代替$\theta^T\theta$，矩阵$M$是根据我们选择的核函数而不同的一个矩阵，这样做可以提高计算效率，修改后的代价函数为：<br>$$C\sum_{i&#x3D;1}^m\left[y^{(i)}Cost_1(\theta^Tf^{(i)})+(1-y^{(i)})Cost_0(\theta^Tf^{(i)})\right]+\frac{1}{2}\theta^TM\theta$$</p>
<blockquote>
<pre><code>理论上讲，我们也可以在逻辑回归中使用核函数，但是上面使用 𝑀来简化计算的方法不适用于逻辑回归，因此计算将非常耗费时间。
</code></pre>
<p>　　在此，我们不介绍最小化支持向量机的代价函数的方法，你可以使用现有的软件包（如<br>liblinear,libsvm 等）。在使用这些软件包最小化我们的代价函数之前，我们通常需要编写核<br>函数，并且如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常必要的。<br>　　另外，支持向量机也可以不使用核函数，不使用核函数又称为线性核函数(linear kernel)，<br>当我们不采用非常复杂的函数，或者我们的训练集特征非常多而实例非常少的时候，可以采<br>用这种不带核函数的支持向量机。<br>　　下面是支持向量机的两个参数𝐶和𝜎的影响：<br>𝐶 &#x3D; 1&#x2F;𝜆<br>𝐶 较大时，相当于𝜆较小，可能会导致过拟合，高方差；<br>𝐶 较小时，相当于𝜆较大，可能会导致低拟合，高偏差；<br>𝜎较大时，可能会导致低方差，高偏差；<br>𝜎较小时，可能会导致低偏差，高方差。<br>来自<a target="_blank" rel="noopener" href="https://www.cnblogs.com/sl0309/p/10499278.html">https://www.cnblogs.com/sl0309/p/10499278.html</a></p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/ccaa256f095359a25441964c2f257396_1745678322868.png" alt="在这里插入图片描述"><br>上图是两个参数对结果的影响</p>
<h2 id="12-6-使用支持向量机-SVM"><a href="#12-6-使用支持向量机-SVM" class="headerlink" title="12-6 使用支持向量机(SVM)"></a>12-6 使用支持向量机(SVM)</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/18ef8b15a24529e666e10c0249a67b67_1745678333602.png" alt="在这里插入图片描述"></p>
<ul>
<li>使用SVM软件库来计算$\theta$的值</li>
<li>我们需要选择C的值</li>
<li>我们需要选择核函数<ul>
<li>线性内核函数：不选择核函数，直接使用线性拟合，可以在特征数量n很大，而样本数量m很小的情况下使用</li>
<li>高斯核函数：可以在样本数量m很大，特征数量n很小的情况下使用，能够拟合出非线性边界</li>
</ul>
</li>
</ul>
<p><em>使用非线性核函数时，需要将特征值归一化</em><br><em>核函数需要满足默塞尔定理Mercer’s theorem</em></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/9a583b6ef87ecb2422be251b7984302d_1745678333602.png" alt="在这里插入图片描述"></p>
<h1 id="13-聚类算法"><a href="#13-聚类算法" class="headerlink" title="13 聚类算法"></a>13 聚类算法</h1><h2 id="13-1-无监督学习"><a href="#13-1-无监督学习" class="headerlink" title="13-1 无监督学习"></a>13-1 无监督学习</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/d766d570598950d3d04a817f11991302_1745678333602.png" alt="在这里插入图片描述"><br>无监督学习的数据集是一堆不带标签的数据，他们没有$y$的值，只有$x$的值</p>
<h2 id="13-2-K均值-K-means-算法"><a href="#13-2-K均值-K-means-算法" class="headerlink" title="13-2 K均值(K-means)算法"></a>13-2 K均值(K-means)算法</h2><p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/13fbab48edb0f16ff02f2676adb1d294_1745678333602.png" alt="在这里插入图片描述"><br>K均值算法的第一步（簇分配）：确定两个聚类中心（图中蓝色叉和红色叉），遍历每一个样本（图中绿点），判断离哪个聚类中心更近，将样本分为两个簇，分完之后如下图<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/bb2563329a0db1fb4d7ab51ff6b2cd12_1745678333602.png" alt="在这里插入图片描述"><br>K均值算法的第二步（移动聚类中心）：计算每一簇中所有点的均值，并将聚类中心移动到该均值处，移动后如下图<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/25acd25d8e9378d277ba8d7cdddc745d_1745678341949.png" alt="在这里插入图片描述"><br>然后再重复第一步判断每一个样本离哪个聚类中心近，并改变他的颜色（分类），改变后再重复第二步。<br>这样不断重复，得到最终结果<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/4c80b124abeedbd56f8a6433dd035f46_1745678341949.png" alt="在这里插入图片描述"><br>这样就可以说K均值已经聚合了<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/12d0a33599587ac3b519ed3e45142db2_1745678341949.png" alt="在这里插入图片描述"><br>输入一个$K$表示想要将数据分为几类，输入不带标签的训练集<br>设训练集是一个n维向量（按照惯例不考虑$x_0&#x3D;1$这一项）<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/cf69a7f1c32bd4d7ecb1405e155b5f15_1745678341949.png" alt="在这里插入图片描述"><br>如上图<br>用$K$表示想要将数据分为$K$类<br>用$\mu_k$表示第$k$个聚类中心的位置（他是一个向量&#x2F;矩阵），随机初始化获得<br>用$c^{(i)}$表示样本中第$i$个点距离最近的那个聚类中心的下标，即第$i$个样本距离第$c^{(i)}$个聚类中心最近，即第$i$个样本属于第$c^{(i)}$个聚类中心，求法如上图中蓝色笔迹<br>求完上述值后，计算每一个聚类中心包含的点的均值，赋值给对应的$\mu_k$，此时已经得到新的聚类中心的位置<br><em>如果有一个没有点的聚类中心，一般直接移除，这样最后会得到K-1类；但如果确实是需要分为K类，那么就将那个没有点的聚类中心重新随机初始化</em><br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/bebb76728303ef962de2539d3b161f40_1745678341949.png" alt="在这里插入图片描述"><br>如上图，有时K均值算法也运用于不能很明显的分类的数据集，如收集了很多人的身高、体重作为数据集，可以看出这些数据基本是连续的，要将其分为S、M、L三类，用聚类算法，也能分为三类。聚类算法也可用于市场的分割</p>
<h2 id="13-3-优化目标"><a href="#13-3-优化目标" class="headerlink" title="13-3 优化目标"></a>13-3 优化目标</h2><p>$\mu_{c^{(i)}}$表示第$i$个样本所属聚类中心的位置<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/549823044586c80c1766c0eddb5dabe0_1745678350134.png" alt="在这里插入图片描述"><br>K均值聚类算法的代价函数（优化目标函数）为<br>$$J(c^{(1)},…,c^{(m)},\mu_1,…,\mu_K)&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^m\Vert x^{(i)}-\mu_{c^{(i)}}\Vert^2$$<br>表示的是每一个样本的位置与它所属的聚类中心位置作差，取范数，再平方，所有共m个样本加起来再求平均数<br>这个代价函数有时也称为失真代价函数(the distortion cost function)或K均值算法的失真</p>
<h2 id="13-4-随机初始化-K均值聚类算法"><a href="#13-4-随机初始化-K均值聚类算法" class="headerlink" title="13-4 随机初始化(K均值聚类算法)"></a>13-4 随机初始化(K均值聚类算法)</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/88e460e0f2673a208ea4876eacb5dff7_1745678350134.png" alt="在这里插入图片描述"></p>
<ul>
<li>从训练集中随机挑选$K$个样本，让第一到第K的聚类中心等于刚刚随机出来的$K$个样本<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/db80227b0cb7e0a9e5e6013e2379e200_1745678350134.png" alt="在这里插入图片描述"><br>如上图，由于是随机挑选的聚类中心，所以结果可能是全局最优（如上图上面的坐标系），也可能落在局部最优上（如上图下面两个坐标系）<br>所以，采用多次随机初始化的方法寻找全局最优<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/5b70b8cd5ea1abeaa28cb2bfc029b8bd_1745678350134.png" alt="在这里插入图片描述"><br>如上图，多次随机初始化的方法为：<br>运行50-1000次K均值聚类算法，可以得到许多不同的代价函数的值，取最小的那个就是最优的聚类<br>如果K&#x3D;2到10，那么多次随机初始化可以明显改进聚类算法效果，如果大于10，多次运行可能不会有特别明显的改善</li>
</ul>
<h2 id="13-5-如何选择聚类数量K"><a href="#13-5-如何选择聚类数量K" class="headerlink" title="13-5 如何选择聚类数量K"></a>13-5 如何选择聚类数量K</h2><p>一般是手动选择<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/79114768e18b1436ba6aff686be27658_1745678350134.png" alt="在这里插入图片描述"></p>
<ul>
<li>如上图，运用肘部法则，坐标系x轴为聚类数量K，坐标系y轴为代价函数的值，绘制曲线后，如左侧的坐标系，能看到曲线在K&#x3D;3处由很高的斜率转为了很低的斜率，这一点认为是“肘部”，选择K&#x3D;3是合适的，但也有可能曲线是如右边坐标系的图像，不能明确地找出合适的聚类数量</li>
</ul>
<p>或者另外一种方法，根据下游目的来手动选择聚类数量</p>
<h1 id="14-降维"><a href="#14-降维" class="headerlink" title="14 降维"></a>14 降维</h1><h2 id="14-1-目标I：数据压缩"><a href="#14-1-目标I：数据压缩" class="headerlink" title="14-1 目标I：数据压缩"></a>14-1 目标I：数据压缩</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/936dac7c80f20e00711be812bc37842b_1745678358571.png" alt="在这里插入图片描述"><br>如上图，将数据从二维压缩为一维，以表示同一个物体的长度为例，$x_1$为用厘米表示，$x_2$为用英尺表示，由于四舍五入产生的误差，坐标系中的样本坐标没有完全练成一条直线，对其进行线性拟合，得到一条直线，让这些点投影在另一条坐标轴$z$上，这样，可以用一个一维的数字$z^{(i)}$来表示原来的一个二维向量$x^{(i)}$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/58d15ca6952e51aa3fb3d6df6b0b467c_1745678358571.png" alt="在这里插入图片描述"><br>如上图，将数据从三维压缩为二维，（在实际应用中可能是将10000维的数据压缩为1000维），空间中所有的点几乎都在一个平面的周围，将所有的点投射到这个平面上，用$z_1$和$z_2$来表示平面的两个坐标轴，这样就把一个三维空间压缩为二维平面，原来的数据用一个二维向量$z^{(i)}$即可表示，$z^{(i)}$中有两个特征$z_1^{(i)}$和$z_2^{(i)}$</p>
<p>降维后的数据可以提高学习算法的运算效率并且节省磁盘存储空间</p>
<h2 id="14-2-目标II：可视化"><a href="#14-2-目标II：可视化" class="headerlink" title="14-2 目标II：可视化"></a>14-2 目标II：可视化</h2><p>一般取k&#x3D;2 or k&#x3D;3来可视化数据集</p>
<h2 id="14-3-主成分分析方法（PCA）"><a href="#14-3-主成分分析方法（PCA）" class="headerlink" title="14-3 主成分分析方法（PCA）"></a>14-3 主成分分析方法（PCA）</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/b62ad25c0519ed0353bb835dcf1d68df_1745678358571.png" alt="在这里插入图片描述"><br>有这样的一个数据集，这个数据集含有二维实数空间内的样本，假设我想对数据进行降维，从二维降到一维，也就是说，我需要找到 一条直线，将数据投影到这条直线上<br>上图中红线是一个不错的选择，因为每个点投影到直线上的距离（蓝线）都很短<br>所以，PCA就是寻找一个低维的东西（在这个例子中是一条直线），让数据投射在上面时的距离的平和最小，这个距离被称为投影误差<br>在使用PCA钱，需要先进行均值归一化和特征规范化，使得特征$x_1$和$x_2$均值为0，数值在可比较的范围之内<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/0b407eb7b686a23a007ab2f93e90ff80_1745678358571.png" alt="在这里插入图片描述"><br>由二维到一维时，找到一个向量即可，三维到二维时，需要找到2个向量组成一个平面，更高维时，需要找到k个向量，让样本投影到这k个向量展开的线性子空间上<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/30a8f2d164b985b796e69bdad9a8db47_1745678358571.png" alt="在这里插入图片描述"><br>上图解释了PCA与线性回归的不同：</p>
<ul>
<li>线性回归是左侧的坐标系，他对一条条竖直的（与y轴平行的）蓝线求和，因为线性回归计算出的误差是指预测的y值与实际y值之间的差</li>
<li>PCA是右侧的坐标系，他对一条条垂直于降维后的直线（在这里是直线）的蓝线求和，因为PCA计算的是实际的点与降维后直线的距离，实际的点是投影上去的</li>
</ul>
<h2 id="14-4-主成分分析算法（PCA）"><a href="#14-4-主成分分析算法（PCA）" class="headerlink" title="14-4 主成分分析算法（PCA）"></a>14-4 主成分分析算法（PCA）</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/f97b5b1f6691ddbcadc9cebd1631aeac_1745678368518.png" alt="在这里插入图片描述"><br>首先进行数据预处理，进行均值标准化，可能要进行特征缩放<br>均值标准化：</p>
<ul>
<li>按照上图先求出某个特征在所有样本中的平均值$\mu_{j}$，公式为$\mu_{j}&#x3D;\frac{1}{m} \sum_{i&#x3D;1}^{m} x_{j}^{(i)}$</li>
<li>然后把每一个旧的$x_{j}^{(i)}$替换成$x_{j}-\mu_{j}$，这样每一个特征的均值都为0<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/995813f8bde9d99192f927b9587a4329_1745678368518.png" alt="在这里插入图片描述"><br>先计算$\Sigma$矩阵（协方差），计算公式为：$\Sigma&#x3D;\frac{1}{m} \sum_{i&#x3D;1}^{n}\left(x^{(i)}\right)\left(x^{(i)}\right)^{T}$，表示为矩阵形式为$\Sigma&#x3D;\frac{1}{m} X^TX$<br>然后用软件库调用svd算法得到矩阵$U$，$U$是一个n×n的矩阵，这里的n&#x3D;m，因为共有n&#x3D;m个样本数量，取矩阵$U$的前k列就是要降维成的k维空间里的k个向量（空间是几维就需要几个向量来表示这个空间，如三维降二维时需要两个向量来表示二维空间）<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/43f797be8fd078c3a24c4d87829d5b6d_1745678368518.png" alt="在这里插入图片描述"><br>如上图，把刚刚取出的k个列向量组成的矩阵命名为$U_{reduce}$，则得到的低维（k维）的数据集$z^{(i)}&#x3D;U_{reduce}^Tx^{(i)}$，该数据集是一个k维向量</li>
</ul>
<h2 id="14-5-压缩重现（解压缩）"><a href="#14-5-压缩重现（解压缩）" class="headerlink" title="14-5 压缩重现（解压缩）"></a>14-5 压缩重现（解压缩）</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/cd89868da97b55901176a061b256cdaa_1745678368518.png" alt="在这里插入图片描述"><br>之前进行了这样的运算：$z&#x3D;U_{reduce}^Tx$<br>其中$z$是新得到的一维向量，$x$是原来的二维向量，$U_{reduce}^T$是通过svd算法得出的<br>现在要恢复二维，进行这样的运算：$x_{appox }&#x3D;U_{ reduee }  z$</p>
<h2 id="14-6-选择主成分数量"><a href="#14-6-选择主成分数量" class="headerlink" title="14-6 选择主成分数量"></a>14-6 选择主成分数量</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/e0adbf4bc6cc7ab5a6909fb58b431caf_1745678368518.png" alt="在这里插入图片描述"><br>上图中分子的式子称为平均平方映射误差，分母称为数据的总变差（它的意思是 “平均来看 我的训练样本 距离零向量多远？ 平均来看 我的训练样本距离原点多远？），分数计算的结果为降维后的新数据与原数据的差距有多大<br>比如假设结果$\le0.01$，则可以说有1%的差异，这个数字比较典型的取值为0.01、0.05、0.10甚至也可能是0.15<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/b20d9cb4b3b43094e0febe460ea14d82_1745678378140.png" alt="在这里插入图片描述"><br>上图左侧是计算合适的k值的方法，这里假设与原数据有小于等于1%的误差<br>可以直接调用svd算法，其中输出的$S$矩阵是一个对角阵<br>用公式$1-\frac{\sum_{i&#x3D;1}^{k} s_{i i}}{\sum_{i&#x3D;1}^{n} s_{i i}} \leqslant 0.01$，直接判断这个公式是否成立即可，找到让这个公式成立的k的值就是合适的k的取值，或者用$\frac{\sum_{i&#x3D;1}^{k} s_{i i}}{\sum_{i&#x3D;1}^{n} s_{ii}} \geqslant 0.99$来判断也是一样的</p>
<ul>
<li>即使要手动选择k值，计算出差异值也可以帮助向别人解释你实现的 PCA 的性能 的一个好方法 ，熟悉 PCA 的人们 就可以通过它 来更好地理解 你用来代表原始数据的 100维数据 近似得有多好 因为有99%的差异性被保留了</li>
</ul>
<h2 id="14-7-应用PCA的建议"><a href="#14-7-应用PCA的建议" class="headerlink" title="14-7 应用PCA的建议"></a>14-7 应用PCA的建议</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/acbacd8c8e2298e5696fd2462f739e01_1745678378140.png" alt="在这里插入图片描述"><br>在使用监督学习时，也可以运用PCA来增加运算效率</p>
<ul>
<li>先将$x^{(1)}, x^{(2)}, \ldots, x^{(m)}$从原来的样本中抽出，运用PCA算法将其降维得到$z^{(1)}, z^{(2)}, \ldots, z^{(m)}$，然后把降维后的$z^{(1)}, z^{(2)}, \ldots, z^{(m)}$替换到原来的样本中，与y一一对应，然后进行监督学习的算法</li>
<li>注意：PCA只能在训练集中使用，不能用于交叉验证集和测试集，从训练集得到了$x$到$z$的对应关系后，可将这个对应关系应用到交叉验证集和测试集</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/6b45ef9fc6a77b4c14f8aa0b4d5cf5df_1745678378140.png" alt="在这里插入图片描述"></p>
<ul>
<li>不要用PCA来防止过拟合，更好的方法是用正则化</li>
<li>PCA是在丢失一定精度的境况下提高运算效率，它在降维时没有与y相关</li>
<li>在使用PCA之前首先尝试使用原数据进行运算，只有在运算速度过慢、占用内存太大、占用磁盘太大、原数据无法成功计算时才使用PCA</li>
</ul>
<h1 id="15-异常检测"><a href="#15-异常检测" class="headerlink" title="15 异常检测"></a>15 异常检测</h1><h2 id="15-1-问题动机"><a href="#15-1-问题动机" class="headerlink" title="15-1 问题动机"></a>15-1 问题动机</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/0e98ecabf89851384d5546522c077d2c_1745678378140.png" alt="在这里插入图片描述"><br>以飞机发动机的异常检测为例，$x_1$和$x_2$分别表示发动机的两个特征，先有一堆数据集表示正常的发动机（如上图红色叉），可以认为越靠近圆圈中间越正常，现在有一个新的发动机$x_{test}$，将他放进坐标系中比较，越靠近中心表示$p\left(x_{\text {test }}\right)$越大，设定一个阈值$\varepsilon$，如果$p\left(x_{\text {test }}\right) \geqslant \varepsilon$则表示该新发动机正常，如果$p\left(x_{\text {test }}\right) &lt; \varepsilon$则表示发动机异常<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/494b8252be0386c281411f1dd043c962_1745678378140.png" alt="在这里插入图片描述"><br>异常检测常被用来进行用户的欺诈监测（检测异常的用户）</p>
<h2 id="15-2-高斯分布（正态分布）"><a href="#15-2-高斯分布（正态分布）" class="headerlink" title="15-2 高斯分布（正态分布）"></a>15-2 高斯分布（正态分布）</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/f9e40d70d6e7b296a467226937aed9e8_1745678388323.png" alt="在这里插入图片描述"></p>
<ul>
<li>$x \sim \mathcal{N}\left(\mu, \sigma^{2}\right)$</li>
<li>$x$是实数，$\sim$表示以…分布，$\mathcal{N}$表示按正态分布，$\mu$表示均值，$\sigma^{2}$表示方差</li>
<li>如上图正态分布曲线（是一条钟形曲线），纵轴表示取到的概率，$\mu$决定曲线中心的位置，标准差$\sigma$决定曲线的宽度</li>
</ul>
<p>高斯分布曲线的公式：<br>$$<br>p\left(x ; \mu, \sigma^{2}\right)<br>&#x3D;\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)<br>$$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/5e24edd97181471ef2b0a8abd3b9ffed_1745678388323.png" alt="在这里插入图片描述"></p>
<ul>
<li>高斯分布曲线的积分&#x3D;1（即上图红色阴影部分面积）</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/d753e09836003dd86356659a76cd6748_1745678388323.png" alt="在这里插入图片描述"><br>如上图，现有一个无标签数据集，可以看出数据集在中间概率大，两侧概率小，假设满足正态分布，进行参数估计<br>假设数据集满足正态分布：$x^{(i)} \sim \mathcal{N}\left(\mu, \sigma^{2}\right)$<br>参数估计公式为：<br>$$\mu&#x3D;\frac{1}{m} \sum_{i&#x3D;1}^{m} x^{(i)}$$<br>$$\sigma^{2}&#x3D;\frac{1}{m} \sum_{i&#x3D;1}^{m}\left(x^{(i)}-\mu\right)^{2}$$<br>求方差的公式中$\frac{1}{m}$也可能是$\frac{1}{m-1}$，在机器学习中，这两种公式差距不大</p>
<h2 id="15-3-算法"><a href="#15-3-算法" class="headerlink" title="15-3 算法"></a>15-3 算法</h2><p>密度估计问题<br>$p(x)$表示特征$x$出现的概率<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/1a9d02da4d3d84cd54f790e905b2388b_1745678388323.png" alt="在这里插入图片描述"><br>假设所有特征均满足正态分布<br>给出概率<br>$$<br>\begin{array}{l}<br>p(x)\<br>&#x3D;p\left(x_{1} ; \mu_{1}, \sigma_{1}^{2}\right) p\left(x_{2} ; \mu_{2}, \sigma_{2}^{2}\right) p\left(x_{3} ; \mu_{3}, \sigma_{3}^{2}\right) \cdots p\left(x_{n} ; \mu_{n}, \sigma_{n}^{2}\right) \<br>&#x3D;\prod_{j&#x3D;1}^{n} p\left(x_{j} ; \mu_{j}, \sigma_{j}^{2}\right)<br>\end{array}<br>$$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/9bf42d1ec3155ea8351ea70f5b9b4552_1745678388323.png" alt="在这里插入图片描述"><br>异常检测算法的流程如上图</p>
<h2 id="15-4-开发和评估异常检测系统"><a href="#15-4-开发和评估异常检测系统" class="headerlink" title="15-4 开发和评估异常检测系统"></a>15-4 开发和评估异常检测系统</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/07c0969bdff61d3d4f6d09f52ff9424e_1745678397914.png" alt="在这里插入图片描述"><br>假设有一个带标签的数据集，选取出正常的样本去掉标签作为训练集，得出异常检测算法，然后用交叉验证集（带标签）来验证，用测试集（带标签）来测试<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/ce0b2078e837665e6cdda613fb727868_1745678397914.png"><br>上图是以发动机检测为例的训练集、交叉验证集和测试集的分法<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/e1b7aca45a53c39bafd710a4cb572bc4_1745678397914.png" alt="在这里插入图片描述"><br>由于数据集中的数据可能非常倾斜，所以可能需要计算F值来判断算法的效果，或者用F值来决定怎样的$\varepsilon$是合适的</p>
<h2 id="15-5-异常检测vs监督学习"><a href="#15-5-异常检测vs监督学习" class="headerlink" title="15-5 异常检测vs监督学习"></a>15-5 异常检测vs监督学习</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/382fbf1ba69b0cdcc58ecfd36ffb19a7_1745678397914.png" alt="在这里插入图片描述"><br>异常检测算法通常用于：</p>
<ul>
<li>有很少数量的正样本（这里以异常的飞机发动机为例），有很大数量的负样本（这里以正常的飞机发动机为例）</li>
<li>学习算法很难从极少数数量的正样本中学习出飞机发动机的异常之处，但能根据负样本学习出正态分布曲线</li>
<li>未来的异常发动机出现的异常特征难以预测，可能与当前的正样本中的异常特征完全不同</li>
</ul>
<p>监督学习算法通常用于：</p>
<ul>
<li>有很大数量的正样本和负样本</li>
<li>有足够的的正样本数量来让学习算法找到特征，并且未来出现的正样本的特征与目前已有的样本的特征相似</li>
</ul>
<p><em>样本数量与某类情况出现概率没关系（即使一类情况出现概率为99%，另一类概率为1%），只要同时有大量的正样本和负样本，就能用监督学习算法，比如癌症预测</em></p>
<h2 id="15-6-选择要使用的特征"><a href="#15-6-选择要使用的特征" class="headerlink" title="15-6 选择要使用的特征"></a>15-6 选择要使用的特征</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/181d506abeca5a8846767a65920c10e2_1745678397914.png" alt="在这里插入图片描述"></p>
<ul>
<li>如果数据像上图第一行的坐标系一样看起来像正态分布的话，可以直接用正态分布</li>
<li>如果数据像上图第二行左侧坐标系，不符合正态分布的话，可以对特征进行转换，比如这里用$log(x)$来替换特征值，让新得到的曲线像正态分布，然后用正态分布</li>
<li>除了用$log(x)$之外，也可以用$log(x+c)$<em>c是常数</em>、$x^{\frac{1}{2}}$、$x^{\frac{1}{3}}$等等<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/4dbf1c3abb8f10ce60fbdf4bece3b946_1745678407077.png" alt="在这里插入图片描述"></li>
<li>如果如上图一个绿色的异常样本混在了正常样本中，算法没能将其挑出来</li>
<li>可以寻找&#x2F;创造一个新的特征让这个异常样本与其他正常样本区别开来<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/311d30da15f9df094ede59a7550e8ce1_1745678407077.png" alt="在这里插入图片描述"><br>举例：<br>服务器CPU负载量和网络流量应该是线性关系的（一起变大，一起变小）<br>如果某一台服务器陷入了死循环，就会导致CPU负载很大而网络流量很小的情况<br>这种情况下要找出异常可以创建一个新的特征$x_5&#x3D;\frac{CPU负载}{网络流量}$，这样就能捕捉到异常。或者是CPU负载的平方也可以</li>
</ul>
<h2 id="15-7-多元高斯分布"><a href="#15-7-多元高斯分布" class="headerlink" title="15-7 多元高斯分布"></a>15-7 多元高斯分布</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/84ea68421ef6995157cd84f2fec45b8b_1745678407077.png" alt="在这里插入图片描述"><br>以服务器的异常为例，上图中绿色样本在两轴中的概率都不低，但很明显在左侧坐标系中他是一个异常样本，算法不能意识到左侧坐标系中的椭圆内的才是正常样本，而默认是以样本中心为圆心，按圆向外概率递减<br>所以要使用多元高斯分布↓<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/2e4f65816dae0c2af08fbba864934ec7_1745678407077.png" alt="在这里插入图片描述"><br>将这两个特征合起来考虑，一个n维向量$\mu$和一个n×n的协方差矩阵$\Sigma$作为多元高斯分布的参数，公式为：<br>$$<br>p(x ; \mu, \Sigma)&#x3D;<br>\frac{1}{(2 \pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(x-\mu)^{\top} \Sigma^{-1}(x-\mu)\right)<br>$$</p>
<ul>
<li>其中$|\Sigma|$是矩阵$\Sigma$的行列式</li>
<li>$\mu$表示的是高斯分布图像中的峰值的位置，这里第一个数是特征$x_1$的位置，第二个数是特征$x_2$的位置</li>
<li>$\mu$和$\Sigma$的计算公式会在下一节给出</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/24ed9c600072a034ea8aa5b2d88ed22d_1745678407077.png%20" alt="在这里插入图片描述"><br>$\mu$对图像的影响如上图所示</p>
<h2 id="15-8-使用多元高斯分布的异常检测"><a href="#15-8-使用多元高斯分布的异常检测" class="headerlink" title="15-8 使用多元高斯分布的异常检测"></a>15-8 使用多元高斯分布的异常检测</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/4f3cc289900b0770442324ddd0bb1716_1745678416844.png" alt="在这里插入图片描述"><br>$\mu$和$\Sigma$的计算公式如上图</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/6ec39ff2f2a7fdf750f3dbbe47e331cd_1745678416844.png" alt="在这里插入图片描述"><br>首先计算出$\mu$和$\Sigma$的值，然后代入到多元高斯分布计算公式，计算出概率，然后将概率与$\varepsilon$比较判断正常or异常<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/e2cee974d33267d16254b1a92d4a3a35_1745678416844.png" alt="在这里插入图片描述"><br>如上图，原来的高斯分布计算出的正态分布图像是一种特殊的多元高斯分布图像，他的椭圆是与轴平行的，而多元高斯分布可以生成一个不与轴平行的椭圆图像<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/bf89274ae7a81d2993ccc4868d118c13_1745678416844.png" alt="在这里插入图片描述"><br>原始高斯分布模型与多元高斯分布的应用区别：</p>
<ul>
<li>原来的可能需要手动创造特征来正确识别异常项，而多元高斯分布不需要，它能自动捕捉这些特征</li>
<li>原来的计算量较小，当特征量达到n&#x3D;10000时就需要用原来的了</li>
<li>原来的可以在样本数量m很小的情况下正常工作，而多元高斯分布需要保证m&gt;&gt;n，一般是m$\ge$ 10n，否则矩阵$\Sigma$会是一个不可逆矩阵</li>
<li>在多元高斯分布中，如果几个特征之间是线性相关的，也会导致矩阵$\Sigma$会是一个不可逆矩阵，比如$x_1&#x3D;x_2$或者$x_1&#x3D;x_2+x_3$，可以通过删除特征$x_1$来解决</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning">机器学习 | Coursera</a><br><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV164411b7dx">吴恩达机器学习系列课程_bilibili</a></p>
</blockquote>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/avatar.jpeg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/avatar.jpeg" title="头像" alt="头像"></a><div class="post-copyright__author_name">Justin</div><div class="post-copyright__author_desc">Hi there!</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://blog.jiaxin.fan/articles/20220130143422.html">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('https://blog.jiaxin.fan/articles/20220130143422.html')">吴恩达机器学习课程笔记 | 第1-15章</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://blog.jiaxin.fan/articles/20220130143422.html"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=吴恩达机器学习课程笔记 | 第1-15章&amp;url=https://blog.jiaxin.fan/articles/20220130143422.html&amp;pic=/img/default_cover.jpg" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.jiaxin.fan" target="_blank">Justin</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/python/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>python<span class="tagsPageCount">14</span></a><a class="post-meta__box__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>人工智能<span class="tagsPageCount">22</span></a><a class="post-meta__box__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>机器学习<span class="tagsPageCount">17</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="/img/default_cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/articles/20220129164049.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Numpy|Python中矩阵和数组乘法及向量相关问题</div></div></a></div><div class="next-post pull-right"><a href="/articles/20220206061609.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">吴恩达机器学习课程笔记第11章</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/articles/20231002061040.html" title="SRCNN复现代码详解"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-10-02</div><div class="title">SRCNN复现代码详解</div></div></a></div><div><a href="/articles/20240224124604.html" title="提取图中苹果的面积、周长和最小外接矩形的python、matlab和c++代码"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2024-02-24</div><div class="title">提取图中苹果的面积、周长和最小外接矩形的python、matlab和c++代码</div></div></a></div><div><a href="/articles/20220209080431.html" title="吴恩达机器学习课程笔记 | 第1-2章"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-02-09</div><div class="title">吴恩达机器学习课程笔记 | 第1-2章</div></div></a></div><div><a href="/articles/20220214220000.html" title="吴恩达机器学习课程笔记 | 第12章"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-02-15</div><div class="title">吴恩达机器学习课程笔记 | 第12章</div></div></a></div><div><a href="/articles/20220216220000.html" title="吴恩达机器学习课程笔记 | 第14章"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-02-17</div><div class="title">吴恩达机器学习课程笔记 | 第14章</div></div></a></div><div><a href="/articles/20220217220000.html" title="吴恩达机器学习课程笔记 | 第15章"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-02-18</div><div class="title">吴恩达机器学习课程笔记 | 第15章</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/24/64e6ce9c507bb.png" alt="status"/></div></div><div class="author-info__description"><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">这有关于<b style="color:#fff">产品、设计、开发</b>相关的问题和看法，还有<b style="color:#fff">文章翻译</b>和<b style="color:#fff">分享</b>。</div><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">相信你可以在这里找到对你有用的<b style="color:#fff">知识</b>和<b style="color:#fff">教程</b>。</div></div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/"><h1 class="author-info__name">Justin</h1><div class="author-info__desc">Hi there!</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://git.jiaxin.fan" target="_blank" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a></div></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bullhorn anzhiyu-shake"></i><span>公告</span></div><div class="announcement_content">欢迎~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">1 介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.</span> <span class="toc-text">1-3 监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.2.</span> <span class="toc-text">1-4 无监督学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">2.</span> <span class="toc-text">2 单变量线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">2-2 代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-Batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">2-5 Batch 梯度下降算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">3.</span> <span class="toc-text">4 多变量线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%A4%9A%E7%89%B9%E5%BE%81"><span class="toc-number">3.1.</span> <span class="toc-text">4-1 多特征</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E5%A4%9A%E5%85%83%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">4-2 多元梯度下降法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E5%A4%9A%E5%85%83%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95I%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="toc-number">3.3.</span> <span class="toc-text">4-3 多元梯度下降法I——特征缩放</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%A9%E6%94%BE"><span class="toc-number">3.3.1.</span> <span class="toc-text">缩放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">3.3.2.</span> <span class="toc-text">归一化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E5%A4%9A%E5%85%83%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%BC%94%E7%BB%83I%E2%80%94%E2%80%94%E5%AD%A6%E4%B9%A0%E7%8E%87%CE%B1"><span class="toc-number">3.4.</span> <span class="toc-text">4-4 多元梯度下降法演练I——学习率α</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="toc-number">3.5.</span> <span class="toc-text">4-5 特征和多项式回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%EF%BC%88Normal-Equation%EF%BC%89%EF%BC%88%E5%8C%BA%E5%88%AB%E4%BA%8E%E8%BF%AD%E4%BB%A3%E6%96%B9%E6%B3%95%E7%9A%84%E7%9B%B4%E6%8E%A5%E8%A7%A3%E6%B3%95%EF%BC%89"><span class="toc-number">3.6.</span> <span class="toc-text">4-6 正规方程（Normal Equation）（区别于迭代方法的直接解法）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">4.</span> <span class="toc-text">6 逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-%E9%80%BB%E8%BE%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text">6-1 逻辑分类算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-%E5%81%87%E8%AE%BE%E8%A1%A8%E7%A4%BA"><span class="toc-number">4.2.</span> <span class="toc-text">6-2 假设表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C"><span class="toc-number">4.3.</span> <span class="toc-text">6-3 决策边界</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">4.4.</span> <span class="toc-text">6-4 代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-%E7%AE%80%E5%8C%96%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">4.5.</span> <span class="toc-text">6-4 简化代价函数与梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96"><span class="toc-number">4.6.</span> <span class="toc-text">6-5 高级优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-7-%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB%EF%BC%9A%E4%B8%80%E5%AF%B9%E5%A4%9A"><span class="toc-number">4.7.</span> <span class="toc-text">6-7 多元分类：一对多</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.</span> <span class="toc-text">7 正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-number">5.1.</span> <span class="toc-text">7-1 过拟合问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-%E6%AD%A3%E5%88%99%E5%8C%96%E6%9B%B4%E6%94%B9-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">5.2.</span> <span class="toc-text">7-2 (正则化更改)代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.3.</span> <span class="toc-text">7-3 线性回归的正则化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-4-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.4.</span> <span class="toc-text">7-4 逻辑回归的正则化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E7%A4%BA"><span class="toc-number">6.</span> <span class="toc-text">8 神经网络的表示</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%81%87%E8%AE%BE"><span class="toc-number">6.1.</span> <span class="toc-text">8-1 非线性假设</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-%E7%A5%9E%E7%BB%8F%E5%85%83%E4%B8%8E%E5%A4%A7%E8%84%91"><span class="toc-number">6.2.</span> <span class="toc-text">8-2 神经元与大脑</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-%E6%A8%A1%E5%9E%8B%E5%B1%95%E7%A4%BAI"><span class="toc-number">6.3.</span> <span class="toc-text">8-3 前向传播-模型展示I</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-%E6%A8%A1%E5%9E%8B%E5%B1%95%E7%A4%BAII"><span class="toc-number">6.4.</span> <span class="toc-text">8-4 前向传播-模型展示II</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-%E4%BE%8B%E5%AD%90%E4%B8%8E%E7%90%86%E8%A7%A3I"><span class="toc-number">6.5.</span> <span class="toc-text">8-5 例子与理解I</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-6-%E4%BE%8B%E5%AD%90%E4%B8%8E%E7%90%86%E8%A7%A3II"><span class="toc-number">6.6.</span> <span class="toc-text">8-6 例子与理解II</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-7-%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB"><span class="toc-number">6.7.</span> <span class="toc-text">8-7 多元分类</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9ALearning"><span class="toc-number">7.</span> <span class="toc-text">9 神经网络：Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-%E5%BA%94%E7%94%A8%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">7.1.</span> <span class="toc-text">9-1 应用于神经网络的代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="toc-number">7.2.</span> <span class="toc-text">9-2 反向传播算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-%E7%90%86%E8%A7%A3%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">7.3.</span> <span class="toc-text">9-3 理解反向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-4-%E5%B1%95%E5%BC%80%E5%8F%82%E6%95%B0"><span class="toc-number">7.4.</span> <span class="toc-text">9-4 展开参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-5-%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%B5%8B"><span class="toc-number">7.5.</span> <span class="toc-text">9-5 梯度检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-6-%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">7.6.</span> <span class="toc-text">9-6 随机初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-7-%E5%9B%9E%E9%A1%BE%E6%80%BB%E7%BB%93"><span class="toc-number">7.7.</span> <span class="toc-text">9-7 回顾总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE"><span class="toc-number">8.</span> <span class="toc-text">10 应用机器学习的建议</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-%E5%86%B3%E5%AE%9A%E4%B8%8B%E4%B8%80%E6%AD%A5%E5%81%9A%E4%BB%80%E4%B9%88"><span class="toc-number">8.1.</span> <span class="toc-text">10-1 决定下一步做什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-2-%E8%AF%84%E4%BC%B0%E5%81%87%E8%AE%BE%E5%87%BD%E6%95%B0"><span class="toc-number">8.2.</span> <span class="toc-text">10-2 评估假设函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-3-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E5%92%8C%E8%AE%AD%E7%BB%83%E3%80%81%E9%AA%8C%E8%AF%81%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">8.3.</span> <span class="toc-text">10-3 模型选择和训练、验证、测试集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-4-%E5%88%A4%E6%96%AD%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE"><span class="toc-number">8.4.</span> <span class="toc-text">10-4 判断偏差与方差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-5-%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E5%81%8F%E5%B7%AE%E3%80%81%E6%96%B9%E5%B7%AE"><span class="toc-number">8.5.</span> <span class="toc-text">10-5 正则化和偏差、方差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-6-%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF"><span class="toc-number">8.6.</span> <span class="toc-text">10-6 学习曲线</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-7-%E5%86%B3%E5%AE%9A%E6%8E%A5%E4%B8%8B%E6%9D%A5%E5%81%9A%E4%BB%80%E4%B9%88"><span class="toc-number">8.7.</span> <span class="toc-text">10-7 决定接下来做什么</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1"><span class="toc-number">9.</span> <span class="toc-text">11 机器学习系统设计</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-1-%E7%A1%AE%E5%AE%9A%E6%89%A7%E8%A1%8C%E7%9A%84%E4%BC%98%E5%85%88%E7%BA%A7%EF%BC%9A%E4%BB%A5%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E4%B8%BA%E4%BE%8B"><span class="toc-number">9.1.</span> <span class="toc-text">11-1 确定执行的优先级：以垃圾邮件分类为例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-2-%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90"><span class="toc-number">9.2.</span> <span class="toc-text">11-2 误差分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-3-%E4%B8%8D%E5%AF%B9%E7%A7%B0%E6%80%A7%E5%88%86%E7%B1%BB%E7%9A%84%E8%AF%AF%E5%B7%AE%E8%AF%84%E4%BC%B0"><span class="toc-number">9.3.</span> <span class="toc-text">11-3 不对称性分类的误差评估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-4-%E6%9F%A5%E5%87%86%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87%E7%9A%84%E5%B9%B3%E8%A1%A1"><span class="toc-number">9.4.</span> <span class="toc-text">11-4 查准率和召回率的平衡</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-5-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE"><span class="toc-number">9.5.</span> <span class="toc-text">11-5 机器学习数据</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89"><span class="toc-number">10.</span> <span class="toc-text">12 支持向量机（SVM）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#12-1-%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-number">10.1.</span> <span class="toc-text">12-1 优化目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-2-%E5%A4%A7%E9%97%B4%E8%B7%9D%E7%9A%84%E7%90%86%E8%A7%A3"><span class="toc-number">10.2.</span> <span class="toc-text">12-2 大间距的理解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-3-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-number">10.3.</span> <span class="toc-text">12-3 支持向量机的数学原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-4-%E6%A0%B8%E5%87%BD%E6%95%B0I"><span class="toc-number">10.4.</span> <span class="toc-text">12-4 核函数I</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-5-%E6%A0%B8%E5%87%BD%E6%95%B0II"><span class="toc-number">10.5.</span> <span class="toc-text">12-5 核函数II</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-6-%E4%BD%BF%E7%94%A8%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM"><span class="toc-number">10.6.</span> <span class="toc-text">12-6 使用支持向量机(SVM)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">11.</span> <span class="toc-text">13 聚类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#13-1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">11.1.</span> <span class="toc-text">13-1 无监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-2-K%E5%9D%87%E5%80%BC-K-means-%E7%AE%97%E6%B3%95"><span class="toc-number">11.2.</span> <span class="toc-text">13-2 K均值(K-means)算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-3-%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-number">11.3.</span> <span class="toc-text">13-3 优化目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-4-%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96-K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">11.4.</span> <span class="toc-text">13-4 随机初始化(K均值聚类算法)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-5-%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E8%81%9A%E7%B1%BB%E6%95%B0%E9%87%8FK"><span class="toc-number">11.5.</span> <span class="toc-text">13-5 如何选择聚类数量K</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#14-%E9%99%8D%E7%BB%B4"><span class="toc-number">12.</span> <span class="toc-text">14 降维</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#14-1-%E7%9B%AE%E6%A0%87I%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9"><span class="toc-number">12.1.</span> <span class="toc-text">14-1 目标I：数据压缩</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-2-%E7%9B%AE%E6%A0%87II%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">12.2.</span> <span class="toc-text">14-2 目标II：可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-3-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95%EF%BC%88PCA%EF%BC%89"><span class="toc-number">12.3.</span> <span class="toc-text">14-3 主成分分析方法（PCA）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-4-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95%EF%BC%88PCA%EF%BC%89"><span class="toc-number">12.4.</span> <span class="toc-text">14-4 主成分分析算法（PCA）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-5-%E5%8E%8B%E7%BC%A9%E9%87%8D%E7%8E%B0%EF%BC%88%E8%A7%A3%E5%8E%8B%E7%BC%A9%EF%BC%89"><span class="toc-number">12.5.</span> <span class="toc-text">14-5 压缩重现（解压缩）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-6-%E9%80%89%E6%8B%A9%E4%B8%BB%E6%88%90%E5%88%86%E6%95%B0%E9%87%8F"><span class="toc-number">12.6.</span> <span class="toc-text">14-6 选择主成分数量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-7-%E5%BA%94%E7%94%A8PCA%E7%9A%84%E5%BB%BA%E8%AE%AE"><span class="toc-number">12.7.</span> <span class="toc-text">14-7 应用PCA的建议</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#15-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B"><span class="toc-number">13.</span> <span class="toc-text">15 异常检测</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#15-1-%E9%97%AE%E9%A2%98%E5%8A%A8%E6%9C%BA"><span class="toc-number">13.1.</span> <span class="toc-text">15-1 问题动机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-2-%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%EF%BC%88%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%EF%BC%89"><span class="toc-number">13.2.</span> <span class="toc-text">15-2 高斯分布（正态分布）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-3-%E7%AE%97%E6%B3%95"><span class="toc-number">13.3.</span> <span class="toc-text">15-3 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-4-%E5%BC%80%E5%8F%91%E5%92%8C%E8%AF%84%E4%BC%B0%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%B3%BB%E7%BB%9F"><span class="toc-number">13.4.</span> <span class="toc-text">15-4 开发和评估异常检测系统</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-5-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8Bvs%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">13.5.</span> <span class="toc-text">15-5 异常检测vs监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-6-%E9%80%89%E6%8B%A9%E8%A6%81%E4%BD%BF%E7%94%A8%E7%9A%84%E7%89%B9%E5%BE%81"><span class="toc-number">13.6.</span> <span class="toc-text">15-6 选择要使用的特征</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-7-%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="toc-number">13.7.</span> <span class="toc-text">15-7 多元高斯分布</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-8-%E4%BD%BF%E7%94%A8%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B"><span class="toc-number">13.8.</span> <span class="toc-text">15-8 使用多元高斯分布的异常检测</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/articles/20250526131427.html" title="Docker部署Overleaf"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Docker部署Overleaf"/></a><div class="content"><a class="title" href="/articles/20250526131427.html" title="Docker部署Overleaf">Docker部署Overleaf</a><time datetime="2025-05-26T13:14:27.000Z" title="发表于 2025-05-26 21:14:27">2025-05-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/articles/20250519132715.html" title="这是一篇用于测试webhook的文章"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="这是一篇用于测试webhook的文章"/></a><div class="content"><a class="title" href="/articles/20250519132715.html" title="这是一篇用于测试webhook的文章">这是一篇用于测试webhook的文章</a><time datetime="2025-05-19T13:27:15.000Z" title="发表于 2025-05-19 21:27:15">2025-05-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/articles/20250428074518.html" title="Hexo博客搭建相关参考"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hexo博客搭建相关参考"/></a><div class="content"><a class="title" href="/articles/20250428074518.html" title="Hexo博客搭建相关参考">Hexo博客搭建相关参考</a><time datetime="2025-04-28T07:45:18.000Z" title="发表于 2025-04-28 15:45:18">2025-04-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/articles/20240224124604.html" title="提取图中苹果的面积、周长和最小外接矩形的python、matlab和c++代码"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="提取图中苹果的面积、周长和最小外接矩形的python、matlab和c++代码"/></a><div class="content"><a class="title" href="/articles/20240224124604.html" title="提取图中苹果的面积、周长和最小外接矩形的python、matlab和c++代码">提取图中苹果的面积、周长和最小外接矩形的python、matlab和c++代码</a><time datetime="2024-02-24T12:46:04.000Z" title="发表于 2024-02-24 20:46:04">2024-02-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/articles/20240224122505.html" title="NVIDIA-SMI has failed because it couldn‘t communicate with the NVIDIA driver报错原因及解决"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/default_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NVIDIA-SMI has failed because it couldn‘t communicate with the NVIDIA driver报错原因及解决"/></a><div class="content"><a class="title" href="/articles/20240224122505.html" title="NVIDIA-SMI has failed because it couldn‘t communicate with the NVIDIA driver报错原因及解决">NVIDIA-SMI has failed because it couldn‘t communicate with the NVIDIA driver报错原因及解决</a><time datetime="2024-02-24T12:25:05.000Z" title="发表于 2024-02-24 20:25:05">2024-02-24</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="footer_deal"><a class="deal_link" href="mailto:hello@jiaxin.fan" title="email"><i class="anzhiyufont anzhiyu-icon-envelope"></i></a><img class="footer_mini_logo" title="返回顶部" alt="返回顶部" onclick="anzhiyu.scrollToDest(0, 500)" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/avatar.jpeg" size="50px"/><a class="deal_link" target="_blank" rel="noopener" href="https://git.jiaxin.fan" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a></div><div id="anzhiyu-footer"><div class="footer-group"><div class="footer-title">服务</div><div class="footer-links"><a class="footer-item" title="51la统计" target="_blank" rel="noopener" href="https://v6.51.la/">51la统计</a><a class="footer-item" title="开往" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html">开往</a></div></div><div class="footer-group"><div class="footer-title">导航</div><div class="footer-links"><a class="footer-item" title="即刻短文" href="/essay/">即刻短文</a><a class="footer-item" title="友链文章" href="/fcircle/">友链文章</a><a class="footer-item" title="留言板" href="/comments/">留言板</a></div></div><div class="footer-group"><div class="footer-title">协议</div><div class="footer-links"><a class="footer-item" title="隐私协议" href="/privacy/">隐私协议</a><a class="footer-item" title="Cookies" href="/cookies/">Cookies</a><a class="footer-item" title="版权协议" href="/copyright/">版权协议</a></div></div><div class="footer-group"><div class="footer-title-group"><div class="footer-title">友链</div><a class="random-friends-btn" id="footer-random-friends-btn" href="javascript:addFriendLinksInFooter();" title="换一批友情链接"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right"></i></a></div><div class="footer-links" id="friend-links-in-footer"></div></div></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo" title="博客框架为Hexo"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Frame-Hexo.svg" alt="博客框架为Hexo"/></a><a class="github-badge" target="_blank" href="https://blog.anheyu.com/" style="margin-inline:5px" data-title="本站使用AnZhiYu主题" title="本站使用AnZhiYu主题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.9/img/Theme-AnZhiYu-2E67D3.svg" alt="本站使用AnZhiYu主题"/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title="本站项目由Github托管"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Source-Github.svg" alt="本站项目由Github托管"/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.2.0/img/badge/Copyright-BY-NC-SA.svg" alt="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"/></a></p></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2021 - 2026 By <a class="footer-bar-link" href="/" title="Justin" target="_blank">Justin</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a><a class="footer-bar-link" target="_blank" rel="noopener" href="https://hexo.io" title="框架">框架</a><a class="footer-bar-link" target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" title="浙ICP备2026000755号">浙ICP备2026000755号</a><a class="footer-bar-link" target="_blank" rel="noopener" href="https://beian.mps.gov.cn/#/query/webSearch?code=33048102260006" title="浙公网安备33048102260006号">浙公网安备33048102260006号</a><a class="footer-bar-link cc" href="/copyright" title="cc协议"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-by-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nc-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nd-line"></i></a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">66</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">60</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">17</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" href="https://blog.jiaxin.fan/" title="我的博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/justinfjx/images/main/hexo/202504281806460.jpeg" alt="我的博客"/><span class="back-menu-item-text">我的博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 时光</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/link/"><span> 导航</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/comments/"><span> 留言板</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/air-conditioner/"><span> 小空调</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="javascript:toRandomPost()"><span> 随便逛逛</span></a></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/Hexo/" style="font-size: 0.88rem;">Hexo<sup>1</sup></a><a href="/tags/anaconda/" style="font-size: 0.88rem;">anaconda<sup>1</sup></a><a href="/tags/axios/" style="font-size: 0.88rem;">axios<sup>1</sup></a><a href="/tags/csdn/" style="font-size: 0.88rem;">csdn<sup>1</sup></a><a href="/tags/cuda/" style="font-size: 0.88rem;">cuda<sup>1</sup></a><a href="/tags/c%E8%AF%AD%E8%A8%80/" style="font-size: 0.88rem;">c语言<sup>3</sup></a><a href="/tags/docker/" style="font-size: 0.88rem;">docker<sup>1</sup></a><a href="/tags/ide/" style="font-size: 0.88rem;">ide<sup>1</sup></a><a href="/tags/latex/" style="font-size: 0.88rem;">latex<sup>1</sup></a><a href="/tags/linux/" style="font-size: 0.88rem;">linux<sup>4</sup></a><a href="/tags/markdown/" style="font-size: 0.88rem;">markdown<sup>1</sup></a><a href="/tags/matlab/" style="font-size: 0.88rem;">matlab<sup>2</sup></a><a href="/tags/numpy/" style="font-size: 0.88rem;">numpy<sup>1</sup></a><a href="/tags/nvidia/" style="font-size: 0.88rem;">nvidia<sup>3</sup></a><a href="/tags/opencv/" style="font-size: 0.88rem;">opencv<sup>3</sup></a><a href="/tags/openmv/" style="font-size: 0.88rem;">openmv<sup>1</sup></a><a href="/tags/openvino/" style="font-size: 0.88rem;">openvino<sup>1</sup></a><a href="/tags/overleaf/" style="font-size: 0.88rem;">overleaf<sup>1</sup></a><a href="/tags/pip/" style="font-size: 0.88rem;">pip<sup>1</sup></a><a href="/tags/pycharm/" style="font-size: 0.88rem;">pycharm<sup>1</sup></a><a href="/tags/python/" style="font-size: 0.88rem;">python<sup>14</sup></a><a href="/tags/pytorch/" style="font-size: 0.88rem;">pytorch<sup>4</sup></a><a href="/tags/stm32/" style="font-size: 0.88rem;">stm32<sup>3</sup></a><a href="/tags/ubuntu/" style="font-size: 0.88rem;">ubuntu<sup>4</sup></a><a href="/tags/vue-axios/" style="font-size: 0.88rem;">vue-axios<sup>1</sup></a><a href="/tags/vue-js/" style="font-size: 0.88rem;">vue.js<sup>2</sup></a><a href="/tags/vue3/" style="font-size: 0.88rem;">vue3<sup>1</sup></a><a href="/tags/windows/" style="font-size: 0.88rem;">windows<sup>4</sup></a><a href="/tags/%E4%B8%B2%E5%8F%A3%E9%80%9A%E4%BF%A1/" style="font-size: 0.88rem;">串口通信<sup>1</sup></a><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 0.88rem;">人工智能<sup>22</sup></a><a href="/tags/%E5%85%B6%E4%BB%96/" style="font-size: 0.88rem;">其他<sup>6</sup></a><a href="/tags/%E5%8D%95%E7%89%87%E6%9C%BA/" style="font-size: 0.88rem;">单片机<sup>2</sup></a><a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 0.88rem;">卷积神经网络<sup>1</sup></a><a href="/tags/%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%A1%AC%E4%BB%B6/" style="font-size: 0.88rem;">嵌入式硬件<sup>2</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">机器学习<sup>17</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">深度学习<sup>9</sup></a><a href="/tags/%E7%9F%A9%E9%98%B5/" style="font-size: 0.88rem;">矩阵<sup>1</sup></a><a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 0.88rem;">神经网络<sup>4</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 0.88rem;">计算机视觉<sup>7</sup></a><a href="/tags/%E9%95%9C%E5%83%8F/" style="font-size: 0.88rem;">镜像<sup>1</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div class="docsearch-wrap"><div id="docsearch" style="display:none"></div><link rel="stylesheet" href="https://cdn.cbd.int/@docsearch/css@3.5.2/dist/style.css"/><script src="https://cdn.cbd.int/@docsearch/js@3.5.2/dist/umd/index.js"></script><script>(() => {
  docsearch(Object.assign({
    appId: 'LQ3PP96P58',
    apiKey: '14355d451d3adf9df136d5d0de8ee169',
    indexName: 'jiaxin',
    container: '#docsearch',
  }, {"insights":true,"container":"#docsearch","debug":false}))

  const handleClick = () => {
    document.querySelector('.DocSearch-Button').click()
  }

  const searchClickFn = () => {
    anzhiyu.addEventListenerPjax(document.querySelector('#search-button > .search'), 'click', handleClick)
  }

  searchClickFn()
  window.addEventListener('pjax:complete', searchClickFn)
})()</script></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2021 By 安知鱼 V1.6.14",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 Justin 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.cbd.int/mathjax@3.2.2/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><input type="hidden" name="page-type" id="page-type" value="post"></div><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div><!-- hexo injector body_end start -->
  <script data-pjax src="https://cdn.jsdelivr.net/gh/barry-flynn/hexo-github-calendar/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://github-calendar-api.meta-code.top/api?user=justinfjx";
            var git_color =['#ebedf0', '#a2f7af', '#6ce480', '#54ad63', '#469252', '#31753c', '#1f5f2a', '#13531f', '#084111', '#032b09', '#000000'];
            var git_user ="justinfjx";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log("已挂载hexo-github-calendar https://github.com/Barry-Flynn/hexo-github-calendar");
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><!-- hexo injector body_end end --></body></html>